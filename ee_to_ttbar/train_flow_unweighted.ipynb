{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.uniform import BoxUniform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "from nflows.transforms.splines.rational_quadratic import rational_quadratic_spline\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import math as m\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard writer for loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU/CPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_RQS_knots = 10   # Number of knots in RQS transform\n",
    "n_made_layers = 1  # Number of hidden layers in every made network\n",
    "n_made_units = 100 # Number of units in every layer of the made network\n",
    "n_flow_layers = 6  # Number of layers in the flow\n",
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 800\n",
    "adam_lr = 0.001     # Learning rate for the ADAM optimizer (default: 0.001)\n",
    "\n",
    "n_train = int(1e6)  # Number of training events\n",
    "n_test = int(1e5)   # Number of testing events\n",
    "n_sample = int(1e6) # Number of samples for ess evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.genfromtxt(\"data/unweighted_samples.csv\", delimiter=',')\n",
    "if (n_train + n_test > samples.shape[0]):\n",
    "    raise Exception(\"Not enough training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = torch.tensor(samples[:n_train], dtype=torch.float32, device=device)\n",
    "test_samples = torch.tensor(samples[n_train:n_train+n_test], dtype=torch.float32, device=device)\n",
    "\n",
    "del samples\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dim = train_samples.shape[1]\n",
    "base_dist = BoxUniform(torch.zeros(event_dim), torch.ones(event_dim))\n",
    "\n",
    "transforms = []\n",
    "for _ in range(n_flow_layers):\n",
    "    transforms.append(RandomPermutation(features=event_dim))\n",
    "    transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "        features=event_dim, \n",
    "        hidden_features=n_made_units,\n",
    "        num_bins=n_RQS_knots,\n",
    "        num_blocks=n_made_layers-1,\n",
    "        tails=\"constrained\",\n",
    "        use_residual_blocks=False\n",
    "    ))\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "optimizer = optim.Adam(flow.parameters(), lr=adam_lr)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[350, 425, 500, 575, 650, 725, 800], gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 batch =  0 / 977 loss =  -1.0884255170822144\n",
      "epoch =  0 batch =  25 / 977 loss =  -2.8339859522306003\n",
      "epoch =  0 batch =  50 / 977 loss =  -4.326266354205561\n",
      "epoch =  0 batch =  75 / 977 loss =  -6.912869434607656\n",
      "epoch =  0 batch =  100 / 977 loss =  -8.86032288145311\n",
      "epoch =  0 batch =  125 / 977 loss =  -10.188018113847765\n",
      "epoch =  0 batch =  150 / 977 loss =  -11.185664401149117\n",
      "epoch =  0 batch =  175 / 977 loss =  -11.996207131580872\n",
      "epoch =  0 batch =  200 / 977 loss =  -12.677316544660881\n",
      "epoch =  0 batch =  225 / 977 loss =  -13.263651326694319\n",
      "epoch =  0 batch =  250 / 977 loss =  -13.77916301294152\n",
      "epoch =  0 batch =  275 / 977 loss =  -14.234175676884856\n",
      "epoch =  0 batch =  300 / 977 loss =  -14.630335736512345\n",
      "epoch =  0 batch =  325 / 977 loss =  -14.975419168823336\n",
      "epoch =  0 batch =  350 / 977 loss =  -15.276764960710139\n",
      "epoch =  0 batch =  375 / 977 loss =  -15.54023440974824\n",
      "epoch =  0 batch =  400 / 977 loss =  -15.775382934960344\n",
      "epoch =  0 batch =  425 / 977 loss =  -15.987931006391284\n",
      "epoch =  0 batch =  450 / 977 loss =  -16.178970096909552\n",
      "epoch =  0 batch =  475 / 977 loss =  -16.354679485329058\n",
      "epoch =  0 batch =  500 / 977 loss =  -16.517568079059473\n",
      "epoch =  0 batch =  525 / 977 loss =  -16.66512801982604\n",
      "epoch =  0 batch =  550 / 977 loss =  -16.79961569815495\n",
      "epoch =  0 batch =  575 / 977 loss =  -16.925321669214302\n",
      "epoch =  0 batch =  600 / 977 loss =  -17.042678713203472\n",
      "epoch =  0 batch =  625 / 977 loss =  -17.15144027079257\n",
      "epoch =  0 batch =  650 / 977 loss =  -17.252204333215996\n",
      "epoch =  0 batch =  675 / 977 loss =  -17.346857458176682\n",
      "epoch =  0 batch =  700 / 977 loss =  -17.434490892924515\n",
      "epoch =  0 batch =  725 / 977 loss =  -17.51773648551016\n",
      "epoch =  0 batch =  750 / 977 loss =  -17.59558004450385\n",
      "epoch =  0 batch =  775 / 977 loss =  -17.66840885902189\n",
      "epoch =  0 batch =  800 / 977 loss =  -17.7359122069141\n",
      "epoch =  0 batch =  825 / 977 loss =  -17.801191062673244\n",
      "epoch =  0 batch =  850 / 977 loss =  -17.861603171508822\n",
      "epoch =  0 batch =  875 / 977 loss =  -17.920204306846347\n",
      "epoch =  0 batch =  900 / 977 loss =  -17.976000171390417\n",
      "epoch =  0 batch =  925 / 977 loss =  -18.029427708096435\n",
      "epoch =  0 batch =  950 / 977 loss =  -18.079328304836306\n",
      "epoch =  0 batch =  975 / 977 loss =  -18.12631304000245\n",
      "Validation loss =  -19.93300437927246\n",
      "epoch =  1 batch =  0 / 977 loss =  -20.077259063720703\n",
      "epoch =  1 batch =  25 / 977 loss =  -19.996413010817307\n",
      "epoch =  1 batch =  50 / 977 loss =  -19.960960088991655\n",
      "epoch =  1 batch =  75 / 977 loss =  -19.96885422656411\n",
      "epoch =  1 batch =  100 / 977 loss =  -19.970630740175153\n",
      "epoch =  1 batch =  125 / 977 loss =  -19.981703758239746\n",
      "epoch =  1 batch =  150 / 977 loss =  -19.982396056320493\n",
      "epoch =  1 batch =  175 / 977 loss =  -19.98616054925052\n",
      "epoch =  1 batch =  200 / 977 loss =  -19.987976339919065\n",
      "epoch =  1 batch =  225 / 977 loss =  -19.989018338971444\n",
      "epoch =  1 batch =  250 / 977 loss =  -19.990660093695055\n",
      "epoch =  1 batch =  275 / 977 loss =  -19.991659634355205\n",
      "epoch =  1 batch =  300 / 977 loss =  -19.99127183958543\n",
      "epoch =  1 batch =  325 / 977 loss =  -19.99342159552079\n",
      "epoch =  1 batch =  350 / 977 loss =  -19.996700232524844\n",
      "epoch =  1 batch =  375 / 977 loss =  -19.997179193699637\n",
      "epoch =  1 batch =  400 / 977 loss =  -19.99924060888126\n",
      "epoch =  1 batch =  425 / 977 loss =  -20.00097992946286\n",
      "epoch =  1 batch =  450 / 977 loss =  -20.002974038642172\n",
      "epoch =  1 batch =  475 / 977 loss =  -20.005436035765335\n",
      "epoch =  1 batch =  500 / 977 loss =  -20.009230394801232\n",
      "epoch =  1 batch =  525 / 977 loss =  -20.011519149229116\n",
      "epoch =  1 batch =  550 / 977 loss =  -20.01469831587833\n",
      "epoch =  1 batch =  575 / 977 loss =  -20.01749971508982\n",
      "epoch =  1 batch =  600 / 977 loss =  -20.021002399742926\n",
      "epoch =  1 batch =  625 / 977 loss =  -20.02374721258976\n",
      "epoch =  1 batch =  650 / 977 loss =  -20.026274811470774\n",
      "epoch =  1 batch =  675 / 977 loss =  -20.0287251162106\n",
      "epoch =  1 batch =  700 / 977 loss =  -20.030828312698368\n",
      "epoch =  1 batch =  725 / 977 loss =  -20.031553560052064\n",
      "epoch =  1 batch =  750 / 977 loss =  -20.033814738815888\n",
      "epoch =  1 batch =  775 / 977 loss =  -20.03563817260194\n",
      "epoch =  1 batch =  800 / 977 loss =  -20.037985183773\n",
      "epoch =  1 batch =  825 / 977 loss =  -20.040931586203243\n",
      "epoch =  1 batch =  850 / 977 loss =  -20.041960560477044\n",
      "epoch =  1 batch =  875 / 977 loss =  -20.043464991600025\n",
      "epoch =  1 batch =  900 / 977 loss =  -20.044788021887868\n",
      "epoch =  1 batch =  925 / 977 loss =  -20.045991759578325\n",
      "epoch =  1 batch =  950 / 977 loss =  -20.047255232256443\n",
      "epoch =  1 batch =  975 / 977 loss =  -20.047682551086947\n",
      "Validation loss =  -20.0740966796875\n",
      "epoch =  2 batch =  0 / 977 loss =  -19.925161361694336\n",
      "epoch =  2 batch =  25 / 977 loss =  -20.09433650970459\n",
      "epoch =  2 batch =  50 / 977 loss =  -20.1283665451349\n",
      "epoch =  2 batch =  75 / 977 loss =  -20.12030159799676\n",
      "epoch =  2 batch =  100 / 977 loss =  -20.1165196824782\n",
      "epoch =  2 batch =  125 / 977 loss =  -20.117105650523353\n",
      "epoch =  2 batch =  150 / 977 loss =  -20.119925151597588\n",
      "epoch =  2 batch =  175 / 977 loss =  -20.117349819703538\n",
      "epoch =  2 batch =  200 / 977 loss =  -20.116815548038012\n",
      "epoch =  2 batch =  225 / 977 loss =  -20.12028770953154\n",
      "epoch =  2 batch =  250 / 977 loss =  -20.12368957549928\n",
      "epoch =  2 batch =  275 / 977 loss =  -20.123935499053076\n",
      "epoch =  2 batch =  300 / 977 loss =  -20.12519383351273\n",
      "epoch =  2 batch =  325 / 977 loss =  -20.12725720083786\n",
      "epoch =  2 batch =  350 / 977 loss =  -20.12678966359195\n",
      "epoch =  2 batch =  375 / 977 loss =  -20.130203896380486\n",
      "epoch =  2 batch =  400 / 977 loss =  -20.132064581511912\n",
      "epoch =  2 batch =  425 / 977 loss =  -20.13187073541918\n",
      "epoch =  2 batch =  450 / 977 loss =  -20.132938681050565\n",
      "epoch =  2 batch =  475 / 977 loss =  -20.134071265949913\n",
      "epoch =  2 batch =  500 / 977 loss =  -20.1349814485409\n",
      "epoch =  2 batch =  525 / 977 loss =  -20.136194660636402\n",
      "epoch =  2 batch =  550 / 977 loss =  -20.138240997674444\n",
      "epoch =  2 batch =  575 / 977 loss =  -20.137050178315885\n",
      "epoch =  2 batch =  600 / 977 loss =  -20.1379552387358\n",
      "epoch =  2 batch =  625 / 977 loss =  -20.13775981500886\n",
      "epoch =  2 batch =  650 / 977 loss =  -20.13878685208508\n",
      "epoch =  2 batch =  675 / 977 loss =  -20.139727287743906\n",
      "epoch =  2 batch =  700 / 977 loss =  -20.14095037027702\n",
      "epoch =  2 batch =  725 / 977 loss =  -20.142249191431283\n",
      "epoch =  2 batch =  750 / 977 loss =  -20.142729054437\n",
      "epoch =  2 batch =  775 / 977 loss =  -20.14281736452554\n",
      "epoch =  2 batch =  800 / 977 loss =  -20.144031381785627\n",
      "epoch =  2 batch =  825 / 977 loss =  -20.145591218592738\n",
      "epoch =  2 batch =  850 / 977 loss =  -20.146734461801156\n",
      "epoch =  2 batch =  875 / 977 loss =  -20.147699876463015\n",
      "epoch =  2 batch =  900 / 977 loss =  -20.14802095707459\n",
      "epoch =  2 batch =  925 / 977 loss =  -20.148011638072845\n",
      "epoch =  2 batch =  950 / 977 loss =  -20.146870353369792\n",
      "epoch =  2 batch =  975 / 977 loss =  -20.147117554164307\n",
      "Validation loss =  -20.178245544433594\n",
      "epoch =  3 batch =  0 / 977 loss =  -20.171863555908203\n",
      "epoch =  3 batch =  25 / 977 loss =  -20.225165073688213\n",
      "epoch =  3 batch =  50 / 977 loss =  -20.19170353459377\n",
      "epoch =  3 batch =  75 / 977 loss =  -20.187552903827868\n",
      "epoch =  3 batch =  100 / 977 loss =  -20.18521095502494\n",
      "epoch =  3 batch =  125 / 977 loss =  -20.183163264441106\n",
      "epoch =  3 batch =  150 / 977 loss =  -20.184282479696716\n",
      "epoch =  3 batch =  175 / 977 loss =  -20.186235525391336\n",
      "epoch =  3 batch =  200 / 977 loss =  -20.18664900936299\n",
      "epoch =  3 batch =  225 / 977 loss =  -20.187247934594634\n",
      "epoch =  3 batch =  250 / 977 loss =  -20.187167072676107\n",
      "epoch =  3 batch =  275 / 977 loss =  -20.18959943799007\n",
      "epoch =  3 batch =  300 / 977 loss =  -20.189068410879766\n",
      "epoch =  3 batch =  325 / 977 loss =  -20.18811236422487\n",
      "epoch =  3 batch =  350 / 977 loss =  -20.186482937587297\n",
      "epoch =  3 batch =  375 / 977 loss =  -20.185294582488687\n",
      "epoch =  3 batch =  400 / 977 loss =  -20.18588188818269\n",
      "epoch =  3 batch =  425 / 977 loss =  -20.186374127025346\n",
      "epoch =  3 batch =  450 / 977 loss =  -20.185586688259498\n",
      "epoch =  3 batch =  475 / 977 loss =  -20.186408824279535\n",
      "epoch =  3 batch =  500 / 977 loss =  -20.186788418097894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  3 batch =  525 / 977 loss =  -20.186301075460346\n",
      "epoch =  3 batch =  550 / 977 loss =  -20.186334661909548\n",
      "epoch =  3 batch =  575 / 977 loss =  -20.18696177999179\n",
      "epoch =  3 batch =  600 / 977 loss =  -20.187734058018336\n",
      "epoch =  3 batch =  625 / 977 loss =  -20.188008902552788\n",
      "epoch =  3 batch =  650 / 977 loss =  -20.187697570261673\n",
      "epoch =  3 batch =  675 / 977 loss =  -20.187590006540507\n",
      "epoch =  3 batch =  700 / 977 loss =  -20.18846712915091\n",
      "epoch =  3 batch =  725 / 977 loss =  -20.189507342567115\n",
      "epoch =  3 batch =  750 / 977 loss =  -20.191064120926306\n",
      "epoch =  3 batch =  775 / 977 loss =  -20.19082364839375\n",
      "epoch =  3 batch =  800 / 977 loss =  -20.192280296678085\n",
      "epoch =  3 batch =  825 / 977 loss =  -20.1935676893368\n",
      "epoch =  3 batch =  850 / 977 loss =  -20.194652978737945\n",
      "epoch =  3 batch =  875 / 977 loss =  -20.19473026223377\n",
      "epoch =  3 batch =  900 / 977 loss =  -20.19576243105261\n",
      "epoch =  3 batch =  925 / 977 loss =  -20.19688218819141\n",
      "epoch =  3 batch =  950 / 977 loss =  -20.196524466625885\n",
      "epoch =  3 batch =  975 / 977 loss =  -20.196908477876992\n",
      "Validation loss =  -20.20844078063965\n"
     ]
    }
   ],
   "source": [
    "data_size = train_samples.shape[0]\n",
    "n_batches = m.ceil(data_size/batch_size)\n",
    "\n",
    "data_size_validation = test_samples.shape[0]\n",
    "n_batches_validate = m.ceil(data_size_validation/batch_size)\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "best_ess = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(data_size, device=device)    \n",
    "\n",
    "    # Loop over batches\n",
    "    cum_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # Set up the batch\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end   = min( (batch+1)*batch_size, data_size-1 )\n",
    "        indices = permutation[batch_begin:batch_end]\n",
    "        samples_batch = train_samples[indices]\n",
    "        \n",
    "        # Take a step\n",
    "        optimizer.zero_grad()\n",
    "        loss = -(flow.log_prob(inputs=samples_batch)).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute cumulative loss\n",
    "        cum_loss = (cum_loss*batch + loss.item())/(batch+1)\n",
    "\n",
    "        if batch%25 == 0:\n",
    "            print(\"epoch = \", epoch, \"batch = \", batch, \"/\", n_batches, \"loss = \", cum_loss)\n",
    "    \n",
    "    writer.add_scalar(\"Loss_train\", cum_loss, epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    # ---------- Compute validation loss -----------\n",
    "    validation_loss = 0\n",
    "    for batch in range(n_batches_validate):\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end = min( (batch+1)*batch_size, data_size_validation-1 )\n",
    "        samples_batch = test_samples[batch_begin:batch_end]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            validation_loss = (validation_loss*batch - (flow.log_prob(samples_batch)).mean())/(batch+1)\n",
    "\n",
    "    print(\"Validation loss = \", validation_loss.item())\n",
    "    writer.add_scalar(\"Loss_test\", validation_loss.item(), epoch)\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        torch.save(flow, \"flow_model_unweighted_best_validation.pt\")\n",
    "        best_validation_loss = validation_loss\n",
    "\n",
    "        \n",
    "torch.save(flow, \"flow_model_unweighted_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
