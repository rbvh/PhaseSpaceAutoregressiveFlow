{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.uniform import BoxUniform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "from nflows.transforms.splines.rational_quadratic import rational_quadratic_spline\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import math as m\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard writer for loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU/CPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_RQS_knots = 10    # Number of knots in RQS transform\n",
    "n_made_layers = 1   # Number of hidden layers in every made network\n",
    "n_made_units = 100  # Number of units in every layer of the made network\n",
    "n_flow_layers = 6   # Number of layers in the flow\n",
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 800\n",
    "adam_lr = 0.001     # Learning rate for the ADAM optimizer (default: 0.001)\n",
    "\n",
    "n_train = int(1e6)  # Number of training events\n",
    "n_test = int(1e5)   # Number of testing events\n",
    "n_sample = int(1e6) # Number of samples for ess evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.genfromtxt(\"data/unweighted_samples.csv\", delimiter=',')\n",
    "if (n_train + n_test > samples.shape[0]):\n",
    "    raise Exception(\"Not enough training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = torch.tensor(samples[:n_train], dtype=torch.float32, device=device)\n",
    "test_samples = torch.tensor(samples[n_train:n_train+n_test], dtype=torch.float32, device=device)\n",
    "\n",
    "del samples\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dim = train_samples.shape[1]\n",
    "base_dist = BoxUniform(torch.zeros(event_dim), torch.ones(event_dim))\n",
    "\n",
    "transforms = []\n",
    "for _ in range(n_flow_layers):\n",
    "    transforms.append(RandomPermutation(features=event_dim))\n",
    "    transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "        features=event_dim, \n",
    "        hidden_features=n_made_units,\n",
    "        num_bins=n_RQS_knots,\n",
    "        num_blocks=n_made_layers-1,\n",
    "        tails=\"constrained\",\n",
    "        use_residual_blocks=False\n",
    "    ))\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "optimizer = optim.Adam(flow.parameters(), lr=adam_lr)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[350, 425, 500, 575, 650, 725, 800], gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 batch =  0 / 977 loss =  0.6082408428192139\n",
      "epoch =  0 batch =  25 / 977 loss =  -4.33077778495275\n",
      "epoch =  0 batch =  50 / 977 loss =  -7.094231059738233\n",
      "epoch =  0 batch =  75 / 977 loss =  -9.647511366950837\n",
      "epoch =  0 batch =  100 / 977 loss =  -11.77341558260493\n",
      "epoch =  0 batch =  125 / 977 loss =  -13.516471081782903\n",
      "epoch =  0 batch =  150 / 977 loss =  -14.866033533945785\n",
      "epoch =  0 batch =  175 / 977 loss =  -15.903519894927745\n",
      "epoch =  0 batch =  200 / 977 loss =  -16.717157538553973\n",
      "epoch =  0 batch =  225 / 977 loss =  -17.37203568826733\n",
      "epoch =  0 batch =  250 / 977 loss =  -17.918119046080147\n",
      "epoch =  0 batch =  275 / 977 loss =  -18.373977463530444\n",
      "epoch =  0 batch =  300 / 977 loss =  -18.766090788516486\n",
      "epoch =  0 batch =  325 / 977 loss =  -19.099831864329197\n",
      "epoch =  0 batch =  350 / 977 loss =  -19.387955561832126\n",
      "epoch =  0 batch =  375 / 977 loss =  -19.646296274947346\n",
      "epoch =  0 batch =  400 / 977 loss =  -19.871072219345614\n",
      "epoch =  0 batch =  425 / 977 loss =  -20.074510383633914\n",
      "epoch =  0 batch =  450 / 977 loss =  -20.25888766272367\n",
      "epoch =  0 batch =  475 / 977 loss =  -20.423137349866998\n",
      "epoch =  0 batch =  500 / 977 loss =  -20.572944128703668\n",
      "epoch =  0 batch =  525 / 977 loss =  -20.710446649291193\n",
      "epoch =  0 batch =  550 / 977 loss =  -20.835535798553092\n",
      "epoch =  0 batch =  575 / 977 loss =  -20.949858080699197\n",
      "epoch =  0 batch =  600 / 977 loss =  -21.054454547692448\n",
      "epoch =  0 batch =  625 / 977 loss =  -21.148712671317217\n",
      "epoch =  0 batch =  650 / 977 loss =  -21.23775113425495\n",
      "epoch =  0 batch =  675 / 977 loss =  -21.31750783527036\n",
      "epoch =  0 batch =  700 / 977 loss =  -21.395168544987616\n",
      "epoch =  0 batch =  725 / 977 loss =  -21.46607836522672\n",
      "epoch =  0 batch =  750 / 977 loss =  -21.532572172056316\n",
      "epoch =  0 batch =  775 / 977 loss =  -21.59674558051158\n",
      "epoch =  0 batch =  800 / 977 loss =  -21.657861967732668\n",
      "epoch =  0 batch =  825 / 977 loss =  -21.715158992255255\n",
      "epoch =  0 batch =  850 / 977 loss =  -21.769644683242237\n",
      "epoch =  0 batch =  875 / 977 loss =  -21.818427247884042\n",
      "epoch =  0 batch =  900 / 977 loss =  -21.867263087752633\n",
      "epoch =  0 batch =  925 / 977 loss =  -21.91363214448768\n",
      "epoch =  0 batch =  950 / 977 loss =  -21.955852738000846\n",
      "epoch =  0 batch =  975 / 977 loss =  -21.997657467901195\n",
      "Validation loss =  -23.552352905273438\n",
      "Effective sample size =  1.29597e-06\n",
      "epoch =  1 batch =  0 / 977 loss =  -23.334796905517578\n",
      "epoch =  1 batch =  25 / 977 loss =  -23.47040865971492\n",
      "epoch =  1 batch =  50 / 977 loss =  -23.519811929440966\n",
      "epoch =  1 batch =  75 / 977 loss =  -23.537948332334818\n",
      "epoch =  1 batch =  100 / 977 loss =  -23.540134108892754\n",
      "epoch =  1 batch =  125 / 977 loss =  -23.549591336931496\n",
      "epoch =  1 batch =  150 / 977 loss =  -23.56660453846912\n",
      "epoch =  1 batch =  175 / 977 loss =  -23.577315655621604\n",
      "epoch =  1 batch =  200 / 977 loss =  -23.572273681412874\n",
      "epoch =  1 batch =  225 / 977 loss =  -23.57578825317652\n",
      "epoch =  1 batch =  250 / 977 loss =  -23.582740540523446\n",
      "epoch =  1 batch =  275 / 977 loss =  -23.582121994184405\n",
      "epoch =  1 batch =  300 / 977 loss =  -23.583360108030195\n",
      "epoch =  1 batch =  325 / 977 loss =  -23.58577890922687\n",
      "epoch =  1 batch =  350 / 977 loss =  -23.5927008984775\n",
      "epoch =  1 batch =  375 / 977 loss =  -23.595103416037055\n",
      "epoch =  1 batch =  400 / 977 loss =  -23.594474402449084\n",
      "epoch =  1 batch =  425 / 977 loss =  -23.59677104770858\n",
      "epoch =  1 batch =  450 / 977 loss =  -23.601360253378456\n",
      "epoch =  1 batch =  475 / 977 loss =  -23.602951506606676\n",
      "epoch =  1 batch =  500 / 977 loss =  -23.605105146914426\n",
      "epoch =  1 batch =  525 / 977 loss =  -23.6032908407001\n",
      "epoch =  1 batch =  550 / 977 loss =  -23.60366338740243\n",
      "epoch =  1 batch =  575 / 977 loss =  -23.60547173354361\n",
      "epoch =  1 batch =  600 / 977 loss =  -23.609859637928484\n",
      "epoch =  1 batch =  625 / 977 loss =  -23.61043241762887\n",
      "epoch =  1 batch =  650 / 977 loss =  -23.611506278979988\n",
      "epoch =  1 batch =  675 / 977 loss =  -23.616227968204665\n",
      "epoch =  1 batch =  700 / 977 loss =  -23.61814417546556\n",
      "epoch =  1 batch =  725 / 977 loss =  -23.619837887031007\n",
      "epoch =  1 batch =  750 / 977 loss =  -23.622821668175362\n",
      "epoch =  1 batch =  775 / 977 loss =  -23.623192944477534\n",
      "epoch =  1 batch =  800 / 977 loss =  -23.62394277403568\n",
      "epoch =  1 batch =  825 / 977 loss =  -23.624639783586794\n",
      "epoch =  1 batch =  850 / 977 loss =  -23.62579830048649\n",
      "epoch =  1 batch =  875 / 977 loss =  -23.62743745002574\n",
      "epoch =  1 batch =  900 / 977 loss =  -23.629689556380107\n",
      "epoch =  1 batch =  925 / 977 loss =  -23.630688459260433\n",
      "epoch =  1 batch =  950 / 977 loss =  -23.632719623553655\n",
      "epoch =  1 batch =  975 / 977 loss =  -23.633993719444916\n",
      "Validation loss =  -23.557781219482422\n",
      "Effective sample size =  0.0893523\n",
      "epoch =  2 batch =  0 / 977 loss =  -23.484102249145508\n",
      "epoch =  2 batch =  25 / 977 loss =  -23.66159307039701\n",
      "epoch =  2 batch =  50 / 977 loss =  -23.663132723639993\n",
      "epoch =  2 batch =  75 / 977 loss =  -23.667684881310713\n",
      "epoch =  2 batch =  100 / 977 loss =  -23.66673420443394\n",
      "epoch =  2 batch =  125 / 977 loss =  -23.676869407532713\n",
      "epoch =  2 batch =  150 / 977 loss =  -23.679610359747677\n",
      "epoch =  2 batch =  175 / 977 loss =  -23.678228410807524\n",
      "epoch =  2 batch =  200 / 977 loss =  -23.67392806627264\n",
      "epoch =  2 batch =  225 / 977 loss =  -23.672232940133693\n",
      "epoch =  2 batch =  250 / 977 loss =  -23.678754487360617\n",
      "epoch =  2 batch =  275 / 977 loss =  -23.683271131653715\n",
      "epoch =  2 batch =  300 / 977 loss =  -23.687170206114295\n",
      "epoch =  2 batch =  325 / 977 loss =  -23.687376976013173\n",
      "epoch =  2 batch =  350 / 977 loss =  -23.682152055267583\n",
      "epoch =  2 batch =  375 / 977 loss =  -23.685747400243223\n",
      "epoch =  2 batch =  400 / 977 loss =  -23.681962386628328\n",
      "epoch =  2 batch =  425 / 977 loss =  -23.68547470021135\n",
      "epoch =  2 batch =  450 / 977 loss =  -23.6882489747853\n",
      "epoch =  2 batch =  475 / 977 loss =  -23.688075414224826\n",
      "epoch =  2 batch =  500 / 977 loss =  -23.690081944722607\n",
      "epoch =  2 batch =  525 / 977 loss =  -23.693203741153376\n",
      "epoch =  2 batch =  550 / 977 loss =  -23.69431865020584\n",
      "epoch =  2 batch =  575 / 977 loss =  -23.692910648054536\n",
      "epoch =  2 batch =  600 / 977 loss =  -23.695021875289417\n",
      "epoch =  2 batch =  625 / 977 loss =  -23.696345932567436\n",
      "epoch =  2 batch =  650 / 977 loss =  -23.69497777349935\n",
      "epoch =  2 batch =  675 / 977 loss =  -23.69596418967612\n",
      "epoch =  2 batch =  700 / 977 loss =  -23.696228092645253\n",
      "epoch =  2 batch =  725 / 977 loss =  -23.69721784933209\n",
      "epoch =  2 batch =  750 / 977 loss =  -23.698498991294464\n",
      "epoch =  2 batch =  775 / 977 loss =  -23.702083432797284\n",
      "epoch =  2 batch =  800 / 977 loss =  -23.7025032853068\n",
      "epoch =  2 batch =  825 / 977 loss =  -23.703152506461137\n",
      "epoch =  2 batch =  850 / 977 loss =  -23.70497352564235\n",
      "epoch =  2 batch =  875 / 977 loss =  -23.707348159459066\n",
      "epoch =  2 batch =  900 / 977 loss =  -23.70770392804245\n",
      "epoch =  2 batch =  925 / 977 loss =  -23.707328454464594\n",
      "epoch =  2 batch =  950 / 977 loss =  -23.70659632035985\n",
      "epoch =  2 batch =  975 / 977 loss =  -23.707932485908742\n",
      "Validation loss =  -23.76145362854004\n"
     ]
    }
   ],
   "source": [
    "data_size = train_samples.shape[0]\n",
    "n_batches = m.ceil(data_size/batch_size)\n",
    "\n",
    "data_size_validation = test_samples.shape[0]\n",
    "n_batches_validate = m.ceil(data_size_validation/batch_size)\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "best_ess = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(data_size, device=device)    \n",
    "\n",
    "    # Loop over batches\n",
    "    cum_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # Set up the batch\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end   = min( (batch+1)*batch_size, data_size-1 )\n",
    "        indices = permutation[batch_begin:batch_end]\n",
    "        samples_batch = train_samples[indices]\n",
    "        \n",
    "        # Take a step\n",
    "        optimizer.zero_grad()\n",
    "        loss = -(flow.log_prob(inputs=samples_batch)).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute cumulative loss\n",
    "        cum_loss = (cum_loss*batch + loss.item())/(batch+1)\n",
    "\n",
    "        if batch%25 == 0:\n",
    "            print(\"epoch = \", epoch, \"batch = \", batch, \"/\", n_batches, \"loss = \", cum_loss)\n",
    "    \n",
    "    writer.add_scalar(\"Loss_train\", cum_loss, epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    # ---------- Compute validation loss -----------\n",
    "    validation_loss = 0\n",
    "    for batch in range(n_batches_validate):\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end = min( (batch+1)*batch_size, data_size_validation-1 )\n",
    "        samples_batch = test_samples[batch_begin:batch_end]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            validation_loss = (validation_loss*batch - (flow.log_prob(samples_batch)).mean())/(batch+1)\n",
    "\n",
    "    print(\"Validation loss = \", validation_loss.item())\n",
    "    writer.add_scalar(\"Loss_test\", validation_loss.item(), epoch)\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        torch.save(flow, \"flow_model_unweighted_best_validation.pt\")\n",
    "        best_validation_loss = validation_loss\n",
    "\n",
    "    \n",
    "    # ---------- Compute effective sample size ----------\n",
    "    # generate samples and evaluate llhs\n",
    "    samples = None\n",
    "    llhs = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "            s = flow.sample(int(n_sample/10))\n",
    "            l = flow.log_prob(s)\n",
    "            if samples is None:\n",
    "                samples, llhs = s.cpu().numpy(), l.cpu().numpy()\n",
    "            else:\n",
    "                samples = np.vstack((samples, s.cpu().numpy()))\n",
    "                llhs = np.vstack((llhs, l.cpu().numpy()))\n",
    "\n",
    "    # Store files\n",
    "    np.savetxt(\"/tmp/samples_file.csv\", samples, delimiter=',')\n",
    "    np.savetxt(\"/tmp/llhs_file.csv\", np.exp(llhs), delimiter=',')\n",
    "\n",
    "    # Run the evaluator\n",
    "    cmd = os.path.abspath(os.getcwd())+'/ME_VEGAS/compute_metrics_from_likelihoods /tmp/samples_file.csv /tmp/llhs_file.csv'\n",
    "    b = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE).stdout\n",
    "    lines = b.decode('ascii').split(\"\\n\")\n",
    "\n",
    "    ess = float(lines[2].split(' ')[-1])\n",
    "    \n",
    "    print(\"Effective sample size = \", ess)\n",
    "    writer.add_scalar(\"Effective_sample_size\", ess, epoch)\n",
    "\n",
    "    if ess > best_ess:\n",
    "        torch.save(flow, \"flow_model_unweighted_best_ess.pt\")\n",
    "        best_ess = ess\n",
    "        \n",
    "torch.save(flow, \"flow_model_unweighted_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
