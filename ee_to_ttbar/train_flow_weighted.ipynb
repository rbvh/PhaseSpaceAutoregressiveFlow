{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.uniform import BoxUniform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "from nflows.transforms.splines.rational_quadratic import rational_quadratic_spline\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import math as m\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard writer for loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU/CPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweighting strategy - must be min, mean or max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_method = 'min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_RQS_knots = 10   # Number of knots in RQS transform\n",
    "n_made_layers = 1  # Number of hidden layers in every made network\n",
    "n_made_units = 100 # Number of units in every layer of the made network\n",
    "n_flow_layers = 6  # Number of layers in the flow\n",
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 800\n",
    "adam_lr = 0.001     # Learning rate for the ADAM optimizer (default: 0.001)\n",
    "\n",
    "n_train = int(1e6)  # Number of training events\n",
    "n_test = int(1e5)   # Number of testing events\n",
    "n_sample = int(1e5) # Number of samples for ess evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train data and reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "train_samples = np.genfromtxt(\"data/weighted_samples.csv\", delimiter=',')[:n_train]\n",
    "train_weights = np.genfromtxt(\"data/weighted_weights.csv\", delimiter=',')[:n_train]\n",
    "\n",
    "# Get reference weight\n",
    "reference_methods = {'min': np.amin, 'mean': np.mean, 'max': np.amax}\n",
    "ref_weight = reference_methods[reference_method](train_weights)\n",
    "\n",
    "# Reweighting and rejection sampling\n",
    "train_weights = train_weights / ref_weight\n",
    "p_rejection_sampling = np.random.rand(len(train_weights))\n",
    "select = p_rejection_sampling < train_weights\n",
    "\n",
    "train_samples = train_samples[select]\n",
    "train_weights = train_weights[select]\n",
    "train_weights[train_weights < 1.0] = 1.0\n",
    "\n",
    "# Normalize weights\n",
    "train_weights /= train_weights.mean()\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_samples = torch.tensor(train_samples, dtype=torch.float32, device=device)\n",
    "train_weights = torch.tensor(train_weights, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor(np.genfromtxt(\"data/unweighted_samples.csv\", delimiter=',')[:n_test], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dim = train_samples.shape[1]\n",
    "base_dist = BoxUniform(torch.zeros(event_dim), torch.ones(event_dim))\n",
    "\n",
    "transforms = []\n",
    "for _ in range(n_flow_layers):\n",
    "    transforms.append(RandomPermutation(features=event_dim))\n",
    "    transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "        features=event_dim, \n",
    "        hidden_features=n_made_units,\n",
    "        num_bins=n_RQS_knots,\n",
    "        num_blocks=n_made_layers-1,\n",
    "        tails=\"constrained\",\n",
    "        use_residual_blocks=False\n",
    "    ))\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "optimizer = optim.Adam(flow.parameters(), lr=adam_lr)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[350, 425, 500, 575, 650, 725, 800], gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 batch =  0 / 977 loss =  0.09872419387102127\n",
      "epoch =  0 batch =  25 / 977 loss =  -4.679852272168947\n",
      "epoch =  0 batch =  50 / 977 loss =  -7.632147923258004\n",
      "epoch =  0 batch =  75 / 977 loss =  -10.231173630509721\n",
      "epoch =  0 batch =  100 / 977 loss =  -12.354916970844906\n",
      "epoch =  0 batch =  125 / 977 loss =  -13.956026222438567\n",
      "epoch =  0 batch =  150 / 977 loss =  -15.142316699373406\n",
      "epoch =  0 batch =  175 / 977 loss =  -16.031893194695428\n",
      "epoch =  0 batch =  200 / 977 loss =  -16.776615907563205\n",
      "epoch =  0 batch =  225 / 977 loss =  -17.389271403539233\n",
      "epoch =  0 batch =  250 / 977 loss =  -17.867114067760358\n",
      "epoch =  0 batch =  275 / 977 loss =  -18.28379206443071\n",
      "epoch =  0 batch =  300 / 977 loss =  -18.64341935186192\n",
      "epoch =  0 batch =  325 / 977 loss =  -18.943766550578044\n",
      "epoch =  0 batch =  350 / 977 loss =  -19.206962892960785\n",
      "epoch =  0 batch =  375 / 977 loss =  -19.452055051427408\n",
      "epoch =  0 batch =  400 / 977 loss =  -19.65662173126032\n",
      "epoch =  0 batch =  425 / 977 loss =  -19.835375757127157\n",
      "epoch =  0 batch =  450 / 977 loss =  -19.997541878397207\n",
      "epoch =  0 batch =  475 / 977 loss =  -20.12844015005193\n",
      "epoch =  0 batch =  500 / 977 loss =  -20.254363501961237\n",
      "epoch =  0 batch =  525 / 977 loss =  -20.370893016386727\n",
      "epoch =  0 batch =  550 / 977 loss =  -20.48527386518171\n",
      "epoch =  0 batch =  575 / 977 loss =  -20.583897642260936\n",
      "epoch =  0 batch =  600 / 977 loss =  -20.682575541890902\n",
      "epoch =  0 batch =  625 / 977 loss =  -20.76671414966354\n",
      "epoch =  0 batch =  650 / 977 loss =  -20.843584651301992\n",
      "epoch =  0 batch =  675 / 977 loss =  -20.922665033368222\n",
      "epoch =  0 batch =  700 / 977 loss =  -20.99721285100288\n",
      "epoch =  0 batch =  725 / 977 loss =  -21.069152040165903\n",
      "epoch =  0 batch =  750 / 977 loss =  -21.12568529078195\n",
      "epoch =  0 batch =  775 / 977 loss =  -21.18342169680509\n",
      "epoch =  0 batch =  800 / 977 loss =  -21.239484576464235\n",
      "epoch =  0 batch =  825 / 977 loss =  -21.29566686226115\n",
      "epoch =  0 batch =  850 / 977 loss =  -21.351328043124873\n",
      "epoch =  0 batch =  875 / 977 loss =  -21.396665979389404\n",
      "epoch =  0 batch =  900 / 977 loss =  -21.443138684792153\n",
      "epoch =  0 batch =  925 / 977 loss =  -21.48964139264778\n",
      "epoch =  0 batch =  950 / 977 loss =  -21.52909955685542\n",
      "epoch =  0 batch =  975 / 977 loss =  -21.573610279579338\n",
      "Validation loss =  -22.800186157226562\n",
      "Effective sample size =  0.123912\n",
      "epoch =  1 batch =  0 / 977 loss =  -23.321029663085938\n",
      "epoch =  1 batch =  25 / 977 loss =  -22.99095183152419\n",
      "epoch =  1 batch =  50 / 977 loss =  -23.10305543039359\n",
      "epoch =  1 batch =  75 / 977 loss =  -23.093619421908727\n",
      "epoch =  1 batch =  100 / 977 loss =  -23.114463031882103\n",
      "epoch =  1 batch =  125 / 977 loss =  -23.108664361257397\n",
      "epoch =  1 batch =  150 / 977 loss =  -23.084365213154168\n",
      "epoch =  1 batch =  175 / 977 loss =  -23.09297101064162\n",
      "epoch =  1 batch =  200 / 977 loss =  -23.089978270269746\n",
      "epoch =  1 batch =  225 / 977 loss =  -23.088461150110295\n",
      "epoch =  1 batch =  250 / 977 loss =  -23.09348107904077\n",
      "epoch =  1 batch =  275 / 977 loss =  -23.089859250662983\n",
      "epoch =  1 batch =  300 / 977 loss =  -23.09327282065965\n",
      "epoch =  1 batch =  325 / 977 loss =  -23.10780818915806\n",
      "epoch =  1 batch =  350 / 977 loss =  -23.10752283949458\n",
      "epoch =  1 batch =  375 / 977 loss =  -23.11186532771334\n",
      "epoch =  1 batch =  400 / 977 loss =  -23.109447208128664\n",
      "epoch =  1 batch =  425 / 977 loss =  -23.09671234077131\n",
      "epoch =  1 batch =  450 / 977 loss =  -23.10009807070713\n",
      "epoch =  1 batch =  475 / 977 loss =  -23.11134982710125\n",
      "epoch =  1 batch =  500 / 977 loss =  -23.119336647901708\n",
      "epoch =  1 batch =  525 / 977 loss =  -23.129843751740545\n",
      "epoch =  1 batch =  550 / 977 loss =  -23.13694356097933\n",
      "epoch =  1 batch =  575 / 977 loss =  -23.139239235056777\n",
      "epoch =  1 batch =  600 / 977 loss =  -23.140099961825104\n",
      "epoch =  1 batch =  625 / 977 loss =  -23.145126656602372\n",
      "epoch =  1 batch =  650 / 977 loss =  -23.14651887720816\n",
      "epoch =  1 batch =  675 / 977 loss =  -23.149837688581496\n",
      "epoch =  1 batch =  700 / 977 loss =  -23.154530959190573\n",
      "epoch =  1 batch =  725 / 977 loss =  -23.16237498774674\n",
      "epoch =  1 batch =  750 / 977 loss =  -23.16194318677393\n",
      "epoch =  1 batch =  775 / 977 loss =  -23.163104895463935\n",
      "epoch =  1 batch =  800 / 977 loss =  -23.163255696290747\n",
      "epoch =  1 batch =  825 / 977 loss =  -23.17167987084563\n",
      "epoch =  1 batch =  850 / 977 loss =  -23.174586969593026\n",
      "epoch =  1 batch =  875 / 977 loss =  -23.175599287634036\n",
      "epoch =  1 batch =  900 / 977 loss =  -23.176679678947636\n",
      "epoch =  1 batch =  925 / 977 loss =  -23.18793565820155\n",
      "epoch =  1 batch =  950 / 977 loss =  -23.188479272098327\n",
      "epoch =  1 batch =  975 / 977 loss =  -23.194345450792163\n",
      "Validation loss =  -23.296443939208984\n",
      "Effective sample size =  0.273867\n",
      "epoch =  2 batch =  0 / 977 loss =  -23.441123962402344\n",
      "epoch =  2 batch =  25 / 977 loss =  -23.282061430124138\n",
      "epoch =  2 batch =  50 / 977 loss =  -23.292374218211453\n",
      "epoch =  2 batch =  75 / 977 loss =  -23.243223064824157\n",
      "epoch =  2 batch =  100 / 977 loss =  -23.25829175439212\n",
      "epoch =  2 batch =  125 / 977 loss =  -23.276393648177862\n",
      "epoch =  2 batch =  150 / 977 loss =  -23.223664252173826\n",
      "epoch =  2 batch =  175 / 977 loss =  -23.244470975615762\n",
      "epoch =  2 batch =  200 / 977 loss =  -23.24440378217555\n",
      "epoch =  2 batch =  225 / 977 loss =  -23.242680625577943\n",
      "epoch =  2 batch =  250 / 977 loss =  -23.238666769992783\n",
      "epoch =  2 batch =  275 / 977 loss =  -23.25374116068301\n",
      "epoch =  2 batch =  300 / 977 loss =  -23.262816679438085\n",
      "epoch =  2 batch =  325 / 977 loss =  -23.270079220730835\n",
      "epoch =  2 batch =  350 / 977 loss =  -23.26466202803827\n",
      "epoch =  2 batch =  375 / 977 loss =  -23.26584386318289\n",
      "epoch =  2 batch =  400 / 977 loss =  -23.26888860193572\n",
      "epoch =  2 batch =  425 / 977 loss =  -23.26055400920027\n",
      "epoch =  2 batch =  450 / 977 loss =  -23.258673674251455\n",
      "epoch =  2 batch =  475 / 977 loss =  -23.264055328208865\n",
      "epoch =  2 batch =  500 / 977 loss =  -23.273219407437573\n",
      "epoch =  2 batch =  525 / 977 loss =  -23.284978036191536\n",
      "epoch =  2 batch =  550 / 977 loss =  -23.28123051804338\n",
      "epoch =  2 batch =  575 / 977 loss =  -23.281654596328746\n",
      "epoch =  2 batch =  600 / 977 loss =  -23.28107620912067\n",
      "epoch =  2 batch =  625 / 977 loss =  -23.286072581720813\n",
      "epoch =  2 batch =  650 / 977 loss =  -23.288540305446737\n",
      "epoch =  2 batch =  675 / 977 loss =  -23.29288109943006\n",
      "epoch =  2 batch =  700 / 977 loss =  -23.288392065595115\n",
      "epoch =  2 batch =  725 / 977 loss =  -23.289948064105243\n",
      "epoch =  2 batch =  750 / 977 loss =  -23.288830204746535\n",
      "epoch =  2 batch =  775 / 977 loss =  -23.292928361401117\n",
      "epoch =  2 batch =  800 / 977 loss =  -23.295456532681925\n",
      "epoch =  2 batch =  825 / 977 loss =  -23.293891479547607\n",
      "epoch =  2 batch =  850 / 977 loss =  -23.29496505593861\n",
      "epoch =  2 batch =  875 / 977 loss =  -23.299350148466623\n",
      "epoch =  2 batch =  900 / 977 loss =  -23.30373186541186\n",
      "epoch =  2 batch =  925 / 977 loss =  -23.302406002069397\n",
      "epoch =  2 batch =  950 / 977 loss =  -23.303028214993162\n",
      "epoch =  2 batch =  975 / 977 loss =  -23.303422081665914\n",
      "Validation loss =  -23.156490325927734\n",
      "Effective sample size =  0.216284\n",
      "epoch =  3 batch =  0 / 977 loss =  -23.96626853942871\n",
      "epoch =  3 batch =  25 / 977 loss =  -23.250871658325195\n",
      "epoch =  3 batch =  50 / 977 loss =  -23.222011715758082\n",
      "epoch =  3 batch =  75 / 977 loss =  -23.29452906156841\n",
      "epoch =  3 batch =  100 / 977 loss =  -23.262318261779182\n",
      "epoch =  3 batch =  125 / 977 loss =  -23.2728945716979\n",
      "epoch =  3 batch =  150 / 977 loss =  -23.273456320857363\n",
      "epoch =  3 batch =  175 / 977 loss =  -23.26868064837022\n",
      "epoch =  3 batch =  200 / 977 loss =  -23.290682958726265\n",
      "epoch =  3 batch =  225 / 977 loss =  -23.286466058376615\n",
      "epoch =  3 batch =  250 / 977 loss =  -23.299977534320725\n",
      "epoch =  3 batch =  275 / 977 loss =  -23.286988797395125\n",
      "epoch =  3 batch =  300 / 977 loss =  -23.30256134489446\n",
      "epoch =  3 batch =  325 / 977 loss =  -23.300295823922188\n",
      "epoch =  3 batch =  350 / 977 loss =  -23.304005120214917\n",
      "epoch =  3 batch =  375 / 977 loss =  -23.309366413887517\n",
      "epoch =  3 batch =  400 / 977 loss =  -23.311498822714032\n",
      "epoch =  3 batch =  425 / 977 loss =  -23.307167048745317\n",
      "epoch =  3 batch =  450 / 977 loss =  -23.309636900535978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  3 batch =  475 / 977 loss =  -23.30830493093539\n",
      "epoch =  3 batch =  500 / 977 loss =  -23.300316386118148\n",
      "epoch =  3 batch =  525 / 977 loss =  -23.304695956154017\n",
      "epoch =  3 batch =  550 / 977 loss =  -23.30738107089338\n",
      "epoch =  3 batch =  575 / 977 loss =  -23.316285620133076\n",
      "epoch =  3 batch =  600 / 977 loss =  -23.31544546041631\n",
      "epoch =  3 batch =  625 / 977 loss =  -23.325154005909877\n",
      "epoch =  3 batch =  650 / 977 loss =  -23.32917614246843\n",
      "epoch =  3 batch =  675 / 977 loss =  -23.329285494674593\n",
      "epoch =  3 batch =  700 / 977 loss =  -23.33381387272507\n",
      "epoch =  3 batch =  725 / 977 loss =  -23.332943112396997\n",
      "epoch =  3 batch =  750 / 977 loss =  -23.342508407470866\n",
      "epoch =  3 batch =  775 / 977 loss =  -23.342431901656475\n",
      "epoch =  3 batch =  800 / 977 loss =  -23.338924467489218\n",
      "epoch =  3 batch =  825 / 977 loss =  -23.335732547769247\n",
      "epoch =  3 batch =  850 / 977 loss =  -23.341551026502312\n",
      "epoch =  3 batch =  875 / 977 loss =  -23.344880426311057\n",
      "epoch =  3 batch =  900 / 977 loss =  -23.346315229375673\n",
      "epoch =  3 batch =  925 / 977 loss =  -23.35034279174475\n",
      "epoch =  3 batch =  950 / 977 loss =  -23.354208812854015\n",
      "epoch =  3 batch =  975 / 977 loss =  -23.349365844101204\n",
      "Validation loss =  -23.386369705200195\n",
      "Effective sample size =  0.306619\n",
      "epoch =  4 batch =  0 / 977 loss =  -24.01970863342285\n",
      "epoch =  4 batch =  25 / 977 loss =  -23.43617057800293\n",
      "epoch =  4 batch =  50 / 977 loss =  -23.399153915106076\n",
      "epoch =  4 batch =  75 / 977 loss =  -23.384489034351546\n",
      "epoch =  4 batch =  100 / 977 loss =  -23.390862002231106\n",
      "epoch =  4 batch =  125 / 977 loss =  -23.409595171610516\n",
      "epoch =  4 batch =  150 / 977 loss =  -23.443054098167167\n",
      "epoch =  4 batch =  175 / 977 loss =  -23.408425667069174\n",
      "epoch =  4 batch =  200 / 977 loss =  -23.401110103474327\n",
      "epoch =  4 batch =  225 / 977 loss =  -23.387364117445145\n",
      "epoch =  4 batch =  250 / 977 loss =  -23.39717747965657\n",
      "epoch =  4 batch =  275 / 977 loss =  -23.389634851096332\n",
      "epoch =  4 batch =  300 / 977 loss =  -23.369418907799204\n",
      "epoch =  4 batch =  325 / 977 loss =  -23.358513346478986\n",
      "epoch =  4 batch =  350 / 977 loss =  -23.344373703002926\n",
      "epoch =  4 batch =  375 / 977 loss =  -23.361094322610406\n",
      "epoch =  4 batch =  400 / 977 loss =  -23.37591432395421\n",
      "epoch =  4 batch =  425 / 977 loss =  -23.37409700922003\n",
      "epoch =  4 batch =  450 / 977 loss =  -23.384031105464416\n",
      "epoch =  4 batch =  475 / 977 loss =  -23.390348682884404\n",
      "epoch =  4 batch =  500 / 977 loss =  -23.386192881418555\n",
      "epoch =  4 batch =  525 / 977 loss =  -23.38722134361702\n",
      "epoch =  4 batch =  550 / 977 loss =  -23.38716413494463\n",
      "epoch =  4 batch =  575 / 977 loss =  -23.38540329866939\n",
      "epoch =  4 batch =  600 / 977 loss =  -23.38697796851743\n",
      "epoch =  4 batch =  625 / 977 loss =  -23.378363374703987\n",
      "epoch =  4 batch =  650 / 977 loss =  -23.376104999430897\n",
      "epoch =  4 batch =  675 / 977 loss =  -23.380991021556966\n",
      "epoch =  4 batch =  700 / 977 loss =  -23.372846529929348\n"
     ]
    }
   ],
   "source": [
    "data_size = train_samples.shape[0]\n",
    "n_batches = m.ceil(data_size/batch_size)\n",
    "\n",
    "data_size_validation = test_samples.shape[0]\n",
    "n_batches_validate = m.ceil(data_size_validation/batch_size)\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "best_ess = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(data_size, device=device)    \n",
    "\n",
    "    # Loop over batches\n",
    "    cum_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # Set up the batch\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end   = min( (batch+1)*batch_size, data_size-1 )\n",
    "        indices = permutation[batch_begin:batch_end]\n",
    "        samples_batch = train_samples[indices]\n",
    "        weights_batch = train_weights[indices]\n",
    "        \n",
    "        # Take a step\n",
    "        optimizer.zero_grad()\n",
    "        loss = -(flow.log_prob(inputs=samples_batch)*weights_batch).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute cumulative loss\n",
    "        cum_loss = (cum_loss*batch + loss.item())/(batch+1)\n",
    "\n",
    "        if batch%25 == 0:\n",
    "            print(\"epoch = \", epoch, \"batch = \", batch, \"/\", n_batches, \"loss = \", cum_loss)\n",
    "    \n",
    "    writer.add_scalar(\"Loss_train\", cum_loss, epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    # ---------- Compute validation loss -----------\n",
    "    validation_loss = 0\n",
    "    for batch in range(n_batches_validate):\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end = min( (batch+1)*batch_size, data_size_validation-1 )\n",
    "        samples_batch = test_samples[batch_begin:batch_end]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            validation_loss = (validation_loss*batch - (flow.log_prob(samples_batch)).mean())/(batch+1)\n",
    "\n",
    "    print(\"Validation loss = \", validation_loss.item())\n",
    "    writer.add_scalar(\"Loss_test\", validation_loss.item(), epoch)\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        torch.save(flow, \"flow_model_weighted_{}_best_validation.pt\".format(reference_method))\n",
    "        best_validation_loss = validation_loss\n",
    "\n",
    "    \n",
    "    # ---------- Compute effective sample size ----------\n",
    "    # generate samples and evaluate llhs\n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(n_sample)\n",
    "        llhs = flow.log_prob(samples)\n",
    "\n",
    "    # Store files\n",
    "    np.savetxt(\"/tmp/samples_weighted_file.csv\", samples.cpu().numpy(), delimiter=',')\n",
    "    np.savetxt(\"/tmp/llhs_weighted_file.csv\", np.exp(llhs.cpu().numpy()), delimiter=',')\n",
    "\n",
    "    # Run the evaluator\n",
    "    cmd = os.path.abspath(os.getcwd())+'/ME_VEGAS/compute_metrics_from_likelihoods /tmp/samples_weighted_file.csv /tmp/llhs_weighted_file.csv'\n",
    "    b = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE).stdout\n",
    "    lines = b.decode('ascii').split(\"\\n\")\n",
    "\n",
    "    ess = float(lines[2].split(' ')[-1])\n",
    "    \n",
    "    print(\"Effective sample size = \", ess)\n",
    "    writer.add_scalar(\"Effective_sample_size\", ess, epoch)\n",
    "\n",
    "    if ess > best_ess:\n",
    "        torch.save(flow, \"flow_model_weighted_{}_best_ess.pt\".format(reference_method))\n",
    "        best_ess = ess\n",
    "        \n",
    "torch.save(flow, \"flow_model_weighted_{}_final.pt\".format(reference_method))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
