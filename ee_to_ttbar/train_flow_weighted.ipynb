{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.uniform import BoxUniform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "from nflows.transforms.splines.rational_quadratic import rational_quadratic_spline\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import math as m\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard writer for loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU/CPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweighting strategy - must be min, mean or max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_method = 'mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_RQS_knots = 10   # Number of knots in RQS transform\n",
    "n_made_layers = 1  # Number of hidden layers in every made network\n",
    "n_made_units = 100 # Number of units in every layer of the made network\n",
    "n_flow_layers = 6  # Number of layers in the flow\n",
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 800\n",
    "adam_lr = 0.001     # Learning rate for the ADAM optimizer (default: 0.001)\n",
    "\n",
    "n_train = int(1e6)  # Number of training events\n",
    "n_test = int(1e5)   # Number of testing events\n",
    "n_sample = int(1e6) # Number of samples for ess evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train data and reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "train_samples = np.genfromtxt(\"data/weighted_samples.csv\", delimiter=',')[:n_train]\n",
    "train_weights = np.genfromtxt(\"data/weighted_weights.csv\", delimiter=',')[:n_train]\n",
    "\n",
    "# Get reference weight\n",
    "reference_methods = {'min': np.amin, 'mean': np.mean, 'max': np.amax}\n",
    "ref_weight = reference_methods[reference_method](train_weights)\n",
    "\n",
    "# Reweighting and rejection sampling\n",
    "train_weights = train_weights / ref_weight\n",
    "p_rejection_sampling = np.random.rand(len(train_weights))\n",
    "select = p_rejection_sampling < train_weights\n",
    "\n",
    "train_samples = train_samples[select]\n",
    "train_weights = train_weights[select]\n",
    "train_weights[train_weights < 1.0] = 1.0\n",
    "\n",
    "# Normalize weights\n",
    "train_weights /= train_weights.mean()\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_samples = torch.tensor(train_samples, dtype=torch.float32, device=device)\n",
    "train_weights = torch.tensor(train_weights, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor(np.genfromtxt(\"data/unweighted_samples.csv\", delimiter=',')[:n_test], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dim = train_samples.shape[1]\n",
    "base_dist = BoxUniform(torch.zeros(event_dim), torch.ones(event_dim))\n",
    "\n",
    "transforms = []\n",
    "for _ in range(n_flow_layers):\n",
    "    transforms.append(RandomPermutation(features=event_dim))\n",
    "    transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "        features=event_dim, \n",
    "        hidden_features=n_made_units,\n",
    "        num_bins=n_RQS_knots,\n",
    "        num_blocks=n_made_layers-1,\n",
    "        tails=\"constrained\",\n",
    "        use_residual_blocks=False\n",
    "    ))\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "optimizer = optim.Adam(flow.parameters(), lr=adam_lr)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[350, 425, 500, 575, 650, 725, 800], gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 batch =  0 / 692 loss =  -1.2549622058868408\n",
      "epoch =  0 batch =  25 / 692 loss =  -3.582621854085189\n",
      "epoch =  0 batch =  50 / 692 loss =  -6.876556239876092\n",
      "epoch =  0 batch =  75 / 692 loss =  -9.507315154138366\n",
      "epoch =  0 batch =  100 / 692 loss =  -11.216482536627515\n",
      "epoch =  0 batch =  125 / 692 loss =  -12.439673572305649\n",
      "epoch =  0 batch =  150 / 692 loss =  -13.423961978084995\n",
      "epoch =  0 batch =  175 / 692 loss =  -14.242115311324598\n",
      "epoch =  0 batch =  200 / 692 loss =  -14.907316291510172\n",
      "epoch =  0 batch =  225 / 692 loss =  -15.444610121503343\n",
      "epoch =  0 batch =  250 / 692 loss =  -15.87897242825345\n",
      "epoch =  0 batch =  275 / 692 loss =  -16.265723538139593\n",
      "epoch =  0 batch =  300 / 692 loss =  -16.592336659811664\n",
      "epoch =  0 batch =  325 / 692 loss =  -16.87967364078651\n",
      "epoch =  0 batch =  350 / 692 loss =  -17.12717241473348\n",
      "epoch =  0 batch =  375 / 692 loss =  -17.35014043367923\n",
      "epoch =  0 batch =  400 / 692 loss =  -17.549229918275376\n",
      "epoch =  0 batch =  425 / 692 loss =  -17.717143313145964\n",
      "epoch =  0 batch =  450 / 692 loss =  -17.879384067529053\n",
      "epoch =  0 batch =  475 / 692 loss =  -18.029491323633355\n",
      "epoch =  0 batch =  500 / 692 loss =  -18.157884479044917\n",
      "epoch =  0 batch =  525 / 692 loss =  -18.278408390714187\n",
      "epoch =  0 batch =  550 / 692 loss =  -18.393019888448627\n",
      "epoch =  0 batch =  575 / 692 loss =  -18.49361946123342\n",
      "epoch =  0 batch =  600 / 692 loss =  -18.587858961148186\n",
      "epoch =  0 batch =  625 / 692 loss =  -18.67589162580502\n",
      "epoch =  0 batch =  650 / 692 loss =  -18.75846517305769\n",
      "epoch =  0 batch =  675 / 692 loss =  -18.83602625607738\n",
      "Validation loss =  -20.803789138793945\n",
      "epoch =  1 batch =  0 / 692 loss =  -21.098054885864258\n",
      "epoch =  1 batch =  25 / 692 loss =  -20.79185992020827\n",
      "epoch =  1 batch =  50 / 692 loss =  -20.84216895757937\n",
      "epoch =  1 batch =  75 / 692 loss =  -20.862736024354632\n",
      "epoch =  1 batch =  100 / 692 loss =  -20.886003456493416\n",
      "epoch =  1 batch =  125 / 692 loss =  -20.866437654646617\n",
      "epoch =  1 batch =  150 / 692 loss =  -20.872668537872507\n",
      "epoch =  1 batch =  175 / 692 loss =  -20.86870727755807\n",
      "epoch =  1 batch =  200 / 692 loss =  -20.874698060068937\n",
      "epoch =  1 batch =  225 / 692 loss =  -20.884602023436976\n",
      "epoch =  1 batch =  250 / 692 loss =  -20.89368196312651\n",
      "epoch =  1 batch =  275 / 692 loss =  -20.89460076456486\n",
      "epoch =  1 batch =  300 / 692 loss =  -20.898744982342404\n",
      "epoch =  1 batch =  325 / 692 loss =  -20.904916020258817\n",
      "epoch =  1 batch =  350 / 692 loss =  -20.905242686258095\n",
      "epoch =  1 batch =  375 / 692 loss =  -20.91359584382241\n",
      "epoch =  1 batch =  400 / 692 loss =  -20.917000102283364\n",
      "epoch =  1 batch =  425 / 692 loss =  -20.92169194825939\n",
      "epoch =  1 batch =  450 / 692 loss =  -20.926301499428092\n",
      "epoch =  1 batch =  475 / 692 loss =  -20.928970909920068\n",
      "epoch =  1 batch =  500 / 692 loss =  -20.934012601475523\n",
      "epoch =  1 batch =  525 / 692 loss =  -20.939943665334035\n",
      "epoch =  1 batch =  550 / 692 loss =  -20.943220924335908\n",
      "epoch =  1 batch =  575 / 692 loss =  -20.948886656098907\n",
      "epoch =  1 batch =  600 / 692 loss =  -20.95589093797021\n",
      "epoch =  1 batch =  625 / 692 loss =  -20.959071098425138\n",
      "epoch =  1 batch =  650 / 692 loss =  -20.959314996745736\n",
      "epoch =  1 batch =  675 / 692 loss =  -20.9605582790262\n",
      "Validation loss =  -20.971813201904297\n",
      "epoch =  2 batch =  0 / 692 loss =  -20.520751953125\n",
      "epoch =  2 batch =  25 / 692 loss =  -21.093862313490646\n",
      "epoch =  2 batch =  50 / 692 loss =  -21.095774557076247\n",
      "epoch =  2 batch =  75 / 692 loss =  -21.079890326449746\n",
      "epoch =  2 batch =  100 / 692 loss =  -21.072787124331644\n",
      "epoch =  2 batch =  125 / 692 loss =  -21.06442336430626\n",
      "epoch =  2 batch =  150 / 692 loss =  -21.053460746411464\n",
      "epoch =  2 batch =  175 / 692 loss =  -21.065811634063724\n",
      "epoch =  2 batch =  200 / 692 loss =  -21.0635900734669\n",
      "epoch =  2 batch =  225 / 692 loss =  -21.067738279832156\n",
      "epoch =  2 batch =  250 / 692 loss =  -21.07027010518717\n",
      "epoch =  2 batch =  275 / 692 loss =  -21.06937247428342\n",
      "epoch =  2 batch =  300 / 692 loss =  -21.06838258001892\n",
      "epoch =  2 batch =  325 / 692 loss =  -21.072380276545438\n",
      "epoch =  2 batch =  350 / 692 loss =  -21.070739300502336\n",
      "epoch =  2 batch =  375 / 692 loss =  -21.071442188100615\n",
      "epoch =  2 batch =  400 / 692 loss =  -21.07561095278163\n",
      "epoch =  2 batch =  425 / 692 loss =  -21.07884535095502\n",
      "epoch =  2 batch =  450 / 692 loss =  -21.08121316660271\n",
      "epoch =  2 batch =  475 / 692 loss =  -21.083029935339926\n",
      "epoch =  2 batch =  500 / 692 loss =  -21.082845238630412\n",
      "epoch =  2 batch =  525 / 692 loss =  -21.087582022518266\n",
      "epoch =  2 batch =  550 / 692 loss =  -21.08859702247025\n",
      "epoch =  2 batch =  575 / 692 loss =  -21.088821520407993\n",
      "epoch =  2 batch =  600 / 692 loss =  -21.091269551815095\n",
      "epoch =  2 batch =  625 / 692 loss =  -21.092624606415875\n",
      "epoch =  2 batch =  650 / 692 loss =  -21.091399349558376\n",
      "epoch =  2 batch =  675 / 692 loss =  -21.09129160536817\n",
      "Validation loss =  -21.146940231323242\n",
      "epoch =  3 batch =  0 / 692 loss =  -21.95999526977539\n",
      "epoch =  3 batch =  25 / 692 loss =  -21.132194005526035\n",
      "epoch =  3 batch =  50 / 692 loss =  -21.146705590042423\n",
      "epoch =  3 batch =  75 / 692 loss =  -21.14795627092061\n",
      "epoch =  3 batch =  100 / 692 loss =  -21.152234105780586\n",
      "epoch =  3 batch =  125 / 692 loss =  -21.168514751252683\n",
      "epoch =  3 batch =  150 / 692 loss =  -21.163801433234816\n",
      "epoch =  3 batch =  175 / 692 loss =  -21.167823683131836\n",
      "epoch =  3 batch =  200 / 692 loss =  -21.167629915683438\n",
      "epoch =  3 batch =  225 / 692 loss =  -21.1625970907971\n",
      "epoch =  3 batch =  250 / 692 loss =  -21.15445364233983\n",
      "epoch =  3 batch =  275 / 692 loss =  -21.143016690793253\n",
      "epoch =  3 batch =  300 / 692 loss =  -21.145133534935237\n",
      "epoch =  3 batch =  325 / 692 loss =  -21.145389902079785\n",
      "epoch =  3 batch =  350 / 692 loss =  -21.14297588054953\n",
      "epoch =  3 batch =  375 / 692 loss =  -21.13997045983661\n",
      "epoch =  3 batch =  400 / 692 loss =  -21.138695695453766\n",
      "epoch =  3 batch =  425 / 692 loss =  -21.139125304602697\n",
      "epoch =  3 batch =  450 / 692 loss =  -21.138292985057635\n",
      "epoch =  3 batch =  475 / 692 loss =  -21.14005254296697\n",
      "epoch =  3 batch =  500 / 692 loss =  -21.140205196753723\n",
      "epoch =  3 batch =  525 / 692 loss =  -21.14018114256769\n",
      "epoch =  3 batch =  550 / 692 loss =  -21.139464670863227\n",
      "epoch =  3 batch =  575 / 692 loss =  -21.141172213686858\n",
      "epoch =  3 batch =  600 / 692 loss =  -21.14101410229475\n",
      "epoch =  3 batch =  625 / 692 loss =  -21.143773246497037\n",
      "epoch =  3 batch =  650 / 692 loss =  -21.142587442002583\n",
      "epoch =  3 batch =  675 / 692 loss =  -21.14650732807865\n",
      "Validation loss =  -21.096965789794922\n"
     ]
    }
   ],
   "source": [
    "data_size = train_samples.shape[0]\n",
    "n_batches = m.ceil(data_size/batch_size)\n",
    "\n",
    "data_size_validation = test_samples.shape[0]\n",
    "n_batches_validate = m.ceil(data_size_validation/batch_size)\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "best_ess = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(data_size, device=device)    \n",
    "\n",
    "    # Loop over batches\n",
    "    cum_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # Set up the batch\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end   = min( (batch+1)*batch_size, data_size-1 )\n",
    "        indices = permutation[batch_begin:batch_end]\n",
    "        samples_batch = train_samples[indices]\n",
    "        weights_batch = train_weights[indices]\n",
    "        \n",
    "        # Take a step\n",
    "        optimizer.zero_grad()\n",
    "        loss = -(flow.log_prob(inputs=samples_batch)*weights_batch).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute cumulative loss\n",
    "        cum_loss = (cum_loss*batch + loss.item())/(batch+1)\n",
    "\n",
    "        if batch%25 == 0:\n",
    "            print(\"epoch = \", epoch, \"batch = \", batch, \"/\", n_batches, \"loss = \", cum_loss)\n",
    "    \n",
    "    writer.add_scalar(\"Loss_train\", cum_loss, epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    # ---------- Compute validation loss -----------\n",
    "    validation_loss = 0\n",
    "    for batch in range(n_batches_validate):\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end = min( (batch+1)*batch_size, data_size_validation-1 )\n",
    "        samples_batch = test_samples[batch_begin:batch_end]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            validation_loss = (validation_loss*batch - (flow.log_prob(samples_batch)).mean())/(batch+1)\n",
    "\n",
    "    print(\"Validation loss = \", validation_loss.item())\n",
    "    writer.add_scalar(\"Loss_test\", validation_loss.item(), epoch)\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        torch.save(flow, \"flow_model_weighted_{}_best_validation.pt\".format(reference_method))\n",
    "        best_validation_loss = validation_loss\n",
    "        \n",
    "torch.save(flow, \"flow_model_weighted_{}_final.pt\".format(reference_method))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
