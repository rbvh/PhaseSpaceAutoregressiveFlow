{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.uniform import BoxUniform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "from nflows.transforms.splines.rational_quadratic import rational_quadratic_spline\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import math as m\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard writer for loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU/CPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reweighting strategy - must be min, mean or max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_method = 'max'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_RQS_knots = 10   # Number of knots in RQS transform\n",
    "n_made_layers = 1  # Number of hidden layers in every made network\n",
    "n_made_units = 100 # Number of units in every layer of the made network\n",
    "n_flow_layers = 6  # Number of layers in the flow\n",
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 800\n",
    "adam_lr = 0.001     # Learning rate for the ADAM optimizer (default: 0.001)\n",
    "\n",
    "n_train = int(1e6)  # Number of training events\n",
    "n_test = int(1e5)   # Number of testing events\n",
    "n_sample = int(1e6) # Number of samples for ess evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train data and reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "train_samples = np.genfromtxt(\"data/weighted_samples.csv\", delimiter=',')[:n_train]\n",
    "train_weights = np.genfromtxt(\"data/weighted_weights.csv\", delimiter=',')[:n_train]\n",
    "\n",
    "# Get reference weight\n",
    "reference_methods = {'min': np.amin, 'mean': np.mean, 'max': np.amax}\n",
    "ref_weight = reference_methods[reference_method](train_weights)\n",
    "\n",
    "# Reweighting and rejection sampling\n",
    "train_weights = train_weights / ref_weight\n",
    "p_rejection_sampling = np.random.rand(len(train_weights))\n",
    "select = p_rejection_sampling < train_weights\n",
    "\n",
    "train_samples = train_samples[select]\n",
    "train_weights = train_weights[select]\n",
    "train_weights[train_weights < 1.0] = 1.0\n",
    "\n",
    "# Normalize weights\n",
    "train_weights /= train_weights.mean()\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_samples = torch.tensor(train_samples, dtype=torch.float32, device=device)\n",
    "train_weights = torch.tensor(train_weights, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor(np.genfromtxt(\"data/unweighted_samples.csv\", delimiter=',')[:n_test], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dim = train_samples.shape[1]\n",
    "base_dist = BoxUniform(torch.zeros(event_dim), torch.ones(event_dim))\n",
    "\n",
    "transforms = []\n",
    "for _ in range(n_flow_layers):\n",
    "    transforms.append(RandomPermutation(features=event_dim))\n",
    "    transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "        features=event_dim, \n",
    "        hidden_features=n_made_units,\n",
    "        num_bins=n_RQS_knots,\n",
    "        num_blocks=n_made_layers-1,\n",
    "        tails=\"constrained\",\n",
    "        use_residual_blocks=False\n",
    "    ))\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "optimizer = optim.Adam(flow.parameters(), lr=adam_lr)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[350, 425, 500, 575, 650, 725, 800], gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 batch =  0 / 63 loss =  -0.4645250141620636\n",
      "epoch =  0 batch =  25 / 63 loss =  -6.297495146210376\n",
      "epoch =  0 batch =  50 / 63 loss =  -10.057873146791087\n",
      "Validation loss =  -18.169567108154297\n",
      "Effective sample size =  0.000225586\n",
      "epoch =  1 batch =  0 / 63 loss =  -18.132858276367188\n",
      "epoch =  1 batch =  25 / 63 loss =  -19.33813711313101\n",
      "epoch =  1 batch =  50 / 63 loss =  -20.12898908876905\n",
      "Validation loss =  -21.515731811523438\n",
      "Effective sample size =  0.0182868\n",
      "epoch =  2 batch =  0 / 63 loss =  -21.639537811279297\n",
      "epoch =  2 batch =  25 / 63 loss =  -21.644849116985615\n",
      "epoch =  2 batch =  50 / 63 loss =  -21.7620132296693\n",
      "Validation loss =  -22.00618553161621\n",
      "Effective sample size =  0.0370023\n",
      "epoch =  3 batch =  0 / 63 loss =  -22.028186798095703\n",
      "epoch =  3 batch =  25 / 63 loss =  -22.100346638606144\n",
      "epoch =  3 batch =  50 / 63 loss =  -22.156181036257276\n",
      "Validation loss =  -22.239023208618164\n",
      "Effective sample size =  0.0489558\n",
      "epoch =  4 batch =  0 / 63 loss =  -22.300094604492188\n",
      "epoch =  4 batch =  25 / 63 loss =  -22.262066987844612\n",
      "epoch =  4 batch =  50 / 63 loss =  -22.30933555902219\n",
      "Validation loss =  -22.404129028320312\n",
      "Effective sample size =  0.0688955\n",
      "epoch =  5 batch =  0 / 63 loss =  -22.447750091552734\n",
      "epoch =  5 batch =  25 / 63 loss =  -22.457277958209698\n",
      "epoch =  5 batch =  50 / 63 loss =  -22.424505532956594\n",
      "Validation loss =  -22.329586029052734\n",
      "Effective sample size =  0.0596652\n",
      "epoch =  6 batch =  0 / 63 loss =  -22.580020904541016\n",
      "epoch =  6 batch =  25 / 63 loss =  -22.451462012070877\n",
      "epoch =  6 batch =  50 / 63 loss =  -22.47955079172172\n",
      "Validation loss =  -22.509794235229492\n",
      "Effective sample size =  0.0750947\n",
      "epoch =  7 batch =  0 / 63 loss =  -22.592350006103516\n",
      "epoch =  7 batch =  25 / 63 loss =  -22.564460607675407\n",
      "epoch =  7 batch =  50 / 63 loss =  -22.563769770603546\n",
      "Validation loss =  -22.33087921142578\n",
      "Effective sample size =  0.0548233\n",
      "epoch =  8 batch =  0 / 63 loss =  -22.487682342529297\n",
      "epoch =  8 batch =  25 / 63 loss =  -22.6313508107112\n",
      "epoch =  8 batch =  50 / 63 loss =  -22.593575346703624\n",
      "Validation loss =  -22.636810302734375\n",
      "Effective sample size =  0.0863171\n",
      "epoch =  9 batch =  0 / 63 loss =  -22.801898956298828\n",
      "epoch =  9 batch =  25 / 63 loss =  -22.670664567213784\n",
      "epoch =  9 batch =  50 / 63 loss =  -22.651337081310793\n",
      "Validation loss =  -22.588960647583008\n",
      "Effective sample size =  0.0756665\n",
      "epoch =  10 batch =  0 / 63 loss =  -22.73870849609375\n",
      "epoch =  10 batch =  25 / 63 loss =  -22.627812165480393\n",
      "epoch =  10 batch =  50 / 63 loss =  -22.661336113424863\n",
      "Validation loss =  -22.734554290771484\n",
      "Effective sample size =  0.105783\n",
      "epoch =  11 batch =  0 / 63 loss =  -22.83527183532715\n",
      "epoch =  11 batch =  25 / 63 loss =  -22.7412931735699\n",
      "epoch =  11 batch =  50 / 63 loss =  -22.757641997991822\n",
      "Validation loss =  -22.643146514892578\n",
      "Effective sample size =  0.0899367\n",
      "epoch =  12 batch =  0 / 63 loss =  -22.480392456054688\n",
      "epoch =  12 batch =  25 / 63 loss =  -22.72594063098614\n",
      "epoch =  12 batch =  50 / 63 loss =  -22.72699655271044\n",
      "Validation loss =  -22.529373168945312\n",
      "Effective sample size =  0.0761818\n",
      "epoch =  13 batch =  0 / 63 loss =  -22.510129928588867\n",
      "epoch =  13 batch =  25 / 63 loss =  -22.6875063089224\n",
      "epoch =  13 batch =  50 / 63 loss =  -22.754154990701114\n",
      "Validation loss =  -22.775928497314453\n",
      "Effective sample size =  0.116242\n",
      "epoch =  14 batch =  0 / 63 loss =  -23.201534271240234\n",
      "epoch =  14 batch =  25 / 63 loss =  -22.797466057997482\n",
      "epoch =  14 batch =  50 / 63 loss =  -22.787132001390646\n",
      "Validation loss =  -22.816547393798828\n",
      "Effective sample size =  0.119035\n",
      "epoch =  15 batch =  0 / 63 loss =  -22.81691551208496\n",
      "epoch =  15 batch =  25 / 63 loss =  -22.875128672673153\n",
      "epoch =  15 batch =  50 / 63 loss =  -22.840466443230124\n",
      "Validation loss =  -22.82231330871582\n",
      "Effective sample size =  0.118105\n",
      "epoch =  16 batch =  0 / 63 loss =  -22.936046600341797\n",
      "epoch =  16 batch =  25 / 63 loss =  -22.86793980231652\n",
      "epoch =  16 batch =  50 / 63 loss =  -22.85662015279134\n",
      "Validation loss =  -22.847793579101562\n",
      "Effective sample size =  0.130878\n",
      "epoch =  17 batch =  0 / 63 loss =  -22.969266891479492\n",
      "epoch =  17 batch =  25 / 63 loss =  -22.842280167799725\n",
      "epoch =  17 batch =  50 / 63 loss =  -22.865080029356715\n",
      "Validation loss =  -22.887784957885742\n",
      "Effective sample size =  0.142344\n",
      "epoch =  18 batch =  0 / 63 loss =  -23.037063598632812\n",
      "epoch =  18 batch =  25 / 63 loss =  -22.87023016122671\n",
      "epoch =  18 batch =  50 / 63 loss =  -22.851117863374597\n",
      "Validation loss =  -22.849227905273438\n",
      "Effective sample size =  0.133691\n",
      "epoch =  19 batch =  0 / 63 loss =  -22.94325065612793\n",
      "epoch =  19 batch =  25 / 63 loss =  -22.887696046095627\n",
      "epoch =  19 batch =  50 / 63 loss =  -22.896687675924863\n",
      "Validation loss =  -22.887956619262695\n",
      "Effective sample size =  0.130297\n",
      "epoch =  20 batch =  0 / 63 loss =  -22.73160171508789\n",
      "epoch =  20 batch =  25 / 63 loss =  -22.89790490957407\n",
      "epoch =  20 batch =  50 / 63 loss =  -22.937025331983378\n",
      "Validation loss =  -22.86562728881836\n",
      "Effective sample size =  0.129034\n",
      "epoch =  21 batch =  0 / 63 loss =  -22.896015167236328\n",
      "epoch =  21 batch =  25 / 63 loss =  -22.896466695345364\n",
      "epoch =  21 batch =  50 / 63 loss =  -22.90655517578126\n",
      "Validation loss =  -22.807283401489258\n",
      "Effective sample size =  0.12013\n",
      "epoch =  22 batch =  0 / 63 loss =  -22.739505767822266\n",
      "epoch =  22 batch =  25 / 63 loss =  -22.89090083195613\n",
      "epoch =  22 batch =  50 / 63 loss =  -22.90647820865407\n",
      "Validation loss =  -22.91463279724121\n",
      "Effective sample size =  0.133264\n",
      "epoch =  23 batch =  0 / 63 loss =  -22.966087341308594\n",
      "epoch =  23 batch =  25 / 63 loss =  -22.945120591383713\n",
      "epoch =  23 batch =  50 / 63 loss =  -22.933259029014447\n",
      "Validation loss =  -22.83316421508789\n",
      "Effective sample size =  0.123585\n",
      "epoch =  24 batch =  0 / 63 loss =  -22.847782135009766\n",
      "epoch =  24 batch =  25 / 63 loss =  -22.955135565537674\n",
      "epoch =  24 batch =  50 / 63 loss =  -22.92758066513959\n",
      "Validation loss =  -22.909875869750977\n",
      "Effective sample size =  0.143729\n",
      "epoch =  25 batch =  0 / 63 loss =  -22.879762649536133\n",
      "epoch =  25 batch =  25 / 63 loss =  -22.940616094149075\n",
      "epoch =  25 batch =  50 / 63 loss =  -22.94074638217103\n",
      "Validation loss =  -22.936063766479492\n",
      "Effective sample size =  0.149914\n",
      "epoch =  26 batch =  0 / 63 loss =  -22.934194564819336\n",
      "epoch =  26 batch =  25 / 63 loss =  -22.936120473421536\n",
      "epoch =  26 batch =  50 / 63 loss =  -22.960490656834022\n",
      "Validation loss =  -22.9189395904541\n",
      "Effective sample size =  0.143616\n",
      "epoch =  27 batch =  0 / 63 loss =  -23.063411712646484\n",
      "epoch =  27 batch =  25 / 63 loss =  -22.992772322434647\n",
      "epoch =  27 batch =  50 / 63 loss =  -22.984350092270795\n",
      "Validation loss =  -22.91404151916504\n",
      "Effective sample size =  0.138022\n",
      "epoch =  28 batch =  0 / 63 loss =  -22.762786865234375\n",
      "epoch =  28 batch =  25 / 63 loss =  -22.91293136890118\n",
      "epoch =  28 batch =  50 / 63 loss =  -22.927913291781557\n",
      "Validation loss =  -22.862306594848633\n",
      "Effective sample size =  0.130935\n",
      "epoch =  29 batch =  0 / 63 loss =  -23.089282989501953\n",
      "epoch =  29 batch =  25 / 63 loss =  -22.958136851970966\n",
      "epoch =  29 batch =  50 / 63 loss =  -23.00059445699056\n",
      "Validation loss =  -22.963802337646484\n",
      "Effective sample size =  0.158146\n",
      "epoch =  30 batch =  0 / 63 loss =  -23.045225143432617\n",
      "epoch =  30 batch =  25 / 63 loss =  -23.018532312833347\n",
      "epoch =  30 batch =  50 / 63 loss =  -22.986391703287754\n",
      "Validation loss =  -22.947689056396484\n",
      "Effective sample size =  0.146659\n",
      "epoch =  31 batch =  0 / 63 loss =  -23.008628845214844\n",
      "epoch =  31 batch =  25 / 63 loss =  -22.99720287322998\n",
      "epoch =  31 batch =  50 / 63 loss =  -22.97988244598988\n",
      "Validation loss =  -22.832754135131836\n",
      "Effective sample size =  0.12598\n",
      "epoch =  32 batch =  0 / 63 loss =  -23.010852813720703\n",
      "epoch =  32 batch =  25 / 63 loss =  -22.91074723463792\n",
      "epoch =  32 batch =  50 / 63 loss =  -22.950424942315795\n",
      "Validation loss =  -22.93741226196289\n",
      "Effective sample size =  0.145164\n",
      "epoch =  33 batch =  0 / 63 loss =  -22.979745864868164\n",
      "epoch =  33 batch =  25 / 63 loss =  -22.977748063894417\n",
      "epoch =  33 batch =  50 / 63 loss =  -23.02520377963197\n",
      "Validation loss =  -23.021875381469727\n",
      "Effective sample size =  0.158891\n",
      "epoch =  34 batch =  0 / 63 loss =  -23.024837493896484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  34 batch =  25 / 63 loss =  -23.03918280968299\n",
      "epoch =  34 batch =  50 / 63 loss =  -23.04936674529431\n",
      "Validation loss =  -22.898420333862305\n",
      "Effective sample size =  0.137145\n",
      "epoch =  35 batch =  0 / 63 loss =  -22.949687957763672\n",
      "epoch =  35 batch =  25 / 63 loss =  -23.03973212608924\n",
      "epoch =  35 batch =  50 / 63 loss =  -23.011451309802485\n",
      "Validation loss =  -22.915550231933594\n",
      "Effective sample size =  0.139481\n",
      "epoch =  36 batch =  0 / 63 loss =  -23.028106689453125\n",
      "epoch =  36 batch =  25 / 63 loss =  -22.96799711080698\n",
      "epoch =  36 batch =  50 / 63 loss =  -22.97469845940085\n",
      "Validation loss =  -22.982295989990234\n",
      "Effective sample size =  0.15824\n",
      "epoch =  37 batch =  0 / 63 loss =  -23.19687843322754\n",
      "epoch =  37 batch =  25 / 63 loss =  -23.013101431039665\n",
      "epoch =  37 batch =  50 / 63 loss =  -23.033437093098964\n",
      "Validation loss =  -22.97371482849121\n",
      "Effective sample size =  0.159378\n",
      "epoch =  38 batch =  0 / 63 loss =  -22.92870330810547\n",
      "epoch =  38 batch =  25 / 63 loss =  -23.04484572777381\n",
      "epoch =  38 batch =  50 / 63 loss =  -23.036661596859204\n",
      "Validation loss =  -23.009279251098633\n",
      "Effective sample size =  0.166142\n",
      "epoch =  39 batch =  0 / 63 loss =  -23.070405960083008\n",
      "epoch =  39 batch =  25 / 63 loss =  -23.024876007666954\n",
      "epoch =  39 batch =  50 / 63 loss =  -23.025899139105103\n",
      "Validation loss =  -23.014909744262695\n",
      "Effective sample size =  0.171129\n",
      "epoch =  40 batch =  0 / 63 loss =  -23.262828826904297\n",
      "epoch =  40 batch =  25 / 63 loss =  -23.035962324876053\n",
      "epoch =  40 batch =  50 / 63 loss =  -23.018556669646614\n",
      "Validation loss =  -23.017513275146484\n",
      "Effective sample size =  0.170108\n",
      "epoch =  41 batch =  0 / 63 loss =  -23.154232025146484\n",
      "epoch =  41 batch =  25 / 63 loss =  -23.042901039123535\n",
      "epoch =  41 batch =  50 / 63 loss =  -23.07134501139323\n",
      "Validation loss =  -23.031696319580078\n",
      "Effective sample size =  0.175319\n",
      "epoch =  42 batch =  0 / 63 loss =  -23.173168182373047\n",
      "epoch =  42 batch =  25 / 63 loss =  -23.04175567626953\n",
      "epoch =  42 batch =  50 / 63 loss =  -23.06156293083639\n",
      "Validation loss =  -23.02338218688965\n",
      "Effective sample size =  0.173697\n",
      "epoch =  43 batch =  0 / 63 loss =  -23.210086822509766\n",
      "epoch =  43 batch =  25 / 63 loss =  -23.067560636080223\n",
      "epoch =  43 batch =  50 / 63 loss =  -23.05333062714221\n",
      "Validation loss =  -22.982128143310547\n",
      "Effective sample size =  0.162641\n",
      "epoch =  44 batch =  0 / 63 loss =  -23.19286346435547\n",
      "epoch =  44 batch =  25 / 63 loss =  -23.06426818554218\n",
      "epoch =  44 batch =  50 / 63 loss =  -23.048852209951363\n",
      "Validation loss =  -22.969087600708008\n",
      "Effective sample size =  0.141962\n",
      "epoch =  45 batch =  0 / 63 loss =  -22.872455596923828\n",
      "epoch =  45 batch =  25 / 63 loss =  -23.064404487609863\n",
      "epoch =  45 batch =  50 / 63 loss =  -23.06046751433728\n",
      "Validation loss =  -23.003511428833008\n",
      "Effective sample size =  0.0365994\n",
      "epoch =  46 batch =  0 / 63 loss =  -23.216800689697266\n",
      "epoch =  46 batch =  25 / 63 loss =  -23.10215950012207\n",
      "epoch =  46 batch =  50 / 63 loss =  -23.08073507570753\n",
      "Validation loss =  -22.960166931152344\n",
      "Effective sample size =  0.152292\n",
      "epoch =  47 batch =  0 / 63 loss =  -22.930200576782227\n",
      "epoch =  47 batch =  25 / 63 loss =  -23.118690197284405\n",
      "epoch =  47 batch =  50 / 63 loss =  -23.088181738759957\n",
      "Validation loss =  -22.97980308532715\n",
      "Effective sample size =  0.150152\n",
      "epoch =  48 batch =  0 / 63 loss =  -22.950963973999023\n",
      "epoch =  48 batch =  25 / 63 loss =  -23.060718609736515\n",
      "epoch =  48 batch =  50 / 63 loss =  -23.066380257700004\n",
      "Validation loss =  -23.052581787109375\n",
      "Effective sample size =  0.184966\n",
      "epoch =  49 batch =  0 / 63 loss =  -23.248157501220703\n",
      "epoch =  49 batch =  25 / 63 loss =  -23.035753396841194\n",
      "epoch =  49 batch =  50 / 63 loss =  -23.070101307887665\n",
      "Validation loss =  -23.007770538330078\n",
      "Effective sample size =  0.168335\n",
      "epoch =  50 batch =  0 / 63 loss =  -22.921859741210938\n",
      "epoch =  50 batch =  25 / 63 loss =  -23.056851093585674\n",
      "epoch =  50 batch =  50 / 63 loss =  -23.055324891034296\n",
      "Validation loss =  -22.931880950927734\n",
      "Effective sample size =  0.150333\n",
      "epoch =  51 batch =  0 / 63 loss =  -22.87484359741211\n",
      "epoch =  51 batch =  25 / 63 loss =  -23.074858812185433\n",
      "epoch =  51 batch =  50 / 63 loss =  -23.091673159131815\n",
      "Validation loss =  -22.902294158935547\n",
      "Effective sample size =  0.137673\n",
      "epoch =  52 batch =  0 / 63 loss =  -23.049989700317383\n",
      "epoch =  52 batch =  25 / 63 loss =  -23.08725467095008\n",
      "epoch =  52 batch =  50 / 63 loss =  -23.097437241498163\n",
      "Validation loss =  -22.89020538330078\n",
      "Effective sample size =  0.120855\n",
      "epoch =  53 batch =  0 / 63 loss =  -22.863990783691406\n",
      "epoch =  53 batch =  25 / 63 loss =  -23.052921368525578\n",
      "epoch =  53 batch =  50 / 63 loss =  -23.063736111510035\n",
      "Validation loss =  -22.95931053161621\n",
      "Effective sample size =  0.149663\n",
      "epoch =  54 batch =  0 / 63 loss =  -23.068706512451172\n",
      "epoch =  54 batch =  25 / 63 loss =  -23.073596220750076\n",
      "epoch =  54 batch =  50 / 63 loss =  -23.066541372560987\n",
      "Validation loss =  -23.033287048339844\n",
      "Effective sample size =  0.174883\n",
      "epoch =  55 batch =  0 / 63 loss =  -23.210037231445312\n",
      "epoch =  55 batch =  25 / 63 loss =  -23.098210408137394\n",
      "epoch =  55 batch =  50 / 63 loss =  -23.097211538576612\n",
      "Validation loss =  -22.990652084350586\n",
      "Effective sample size =  0.162086\n",
      "epoch =  56 batch =  0 / 63 loss =  -23.014972686767578\n",
      "epoch =  56 batch =  25 / 63 loss =  -23.09032902350793\n",
      "epoch =  56 batch =  50 / 63 loss =  -23.093526204427093\n",
      "Validation loss =  -22.992305755615234\n",
      "Effective sample size =  0.163503\n",
      "epoch =  57 batch =  0 / 63 loss =  -23.10346031188965\n",
      "epoch =  57 batch =  25 / 63 loss =  -23.078194178067722\n",
      "epoch =  57 batch =  50 / 63 loss =  -23.069013932172\n",
      "Validation loss =  -22.863765716552734\n",
      "Effective sample size =  0.129395\n",
      "epoch =  58 batch =  0 / 63 loss =  -22.967945098876953\n",
      "epoch =  58 batch =  25 / 63 loss =  -23.106577066274788\n",
      "epoch =  58 batch =  50 / 63 loss =  -23.08353461471258\n",
      "Validation loss =  -22.96296501159668\n",
      "Effective sample size =  0.155041\n",
      "epoch =  59 batch =  0 / 63 loss =  -22.96751594543457\n",
      "epoch =  59 batch =  25 / 63 loss =  -23.071476276104267\n",
      "epoch =  59 batch =  50 / 63 loss =  -23.10114673539704\n",
      "Validation loss =  -23.004541397094727\n",
      "Effective sample size =  0.162302\n",
      "epoch =  60 batch =  0 / 63 loss =  -23.065425872802734\n",
      "epoch =  60 batch =  25 / 63 loss =  -23.124440780052772\n",
      "epoch =  60 batch =  50 / 63 loss =  -23.107682321585862\n",
      "Validation loss =  -23.061290740966797\n",
      "Effective sample size =  0.182055\n",
      "epoch =  61 batch =  0 / 63 loss =  -23.268314361572266\n",
      "epoch =  61 batch =  25 / 63 loss =  -23.14530504666842\n",
      "epoch =  61 batch =  50 / 63 loss =  -23.13725344340006\n",
      "Validation loss =  -23.019411087036133\n",
      "Effective sample size =  0.145147\n",
      "epoch =  62 batch =  0 / 63 loss =  -23.148107528686523\n",
      "epoch =  62 batch =  25 / 63 loss =  -23.10257126734807\n",
      "epoch =  62 batch =  50 / 63 loss =  -23.088519077675016\n",
      "Validation loss =  -22.97930908203125\n",
      "Effective sample size =  0.142913\n",
      "epoch =  63 batch =  0 / 63 loss =  -23.218387603759766\n",
      "epoch =  63 batch =  25 / 63 loss =  -23.08522759951078\n",
      "epoch =  63 batch =  50 / 63 loss =  -23.097600787293675\n",
      "Validation loss =  -22.999174118041992\n",
      "Effective sample size =  0.16076\n",
      "epoch =  64 batch =  0 / 63 loss =  -23.319978713989258\n",
      "epoch =  64 batch =  25 / 63 loss =  -23.073420524597168\n",
      "epoch =  64 batch =  50 / 63 loss =  -23.0896032370773\n",
      "Validation loss =  -23.026037216186523\n",
      "Effective sample size =  0.177174\n",
      "epoch =  65 batch =  0 / 63 loss =  -22.962589263916016\n",
      "epoch =  65 batch =  25 / 63 loss =  -23.065314439626846\n",
      "epoch =  65 batch =  50 / 63 loss =  -23.0915792128619\n",
      "Validation loss =  -22.920856475830078\n",
      "Effective sample size =  0.141705\n",
      "epoch =  66 batch =  0 / 63 loss =  -23.12430191040039\n",
      "epoch =  66 batch =  25 / 63 loss =  -23.12203128521259\n",
      "epoch =  66 batch =  50 / 63 loss =  -23.10813720553529\n",
      "Validation loss =  -23.032129287719727\n",
      "Effective sample size =  0.174344\n",
      "epoch =  67 batch =  0 / 63 loss =  -23.1021728515625\n",
      "epoch =  67 batch =  25 / 63 loss =  -23.10418393061711\n",
      "epoch =  67 batch =  50 / 63 loss =  -23.110379836138556\n",
      "Validation loss =  -23.047290802001953\n",
      "Effective sample size =  0.176161\n",
      "epoch =  68 batch =  0 / 63 loss =  -23.15273666381836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  68 batch =  25 / 63 loss =  -23.130938383249138\n",
      "epoch =  68 batch =  50 / 63 loss =  -23.12338241876341\n",
      "Validation loss =  -23.010623931884766\n",
      "Effective sample size =  0.164685\n",
      "epoch =  69 batch =  0 / 63 loss =  -23.307445526123047\n",
      "epoch =  69 batch =  25 / 63 loss =  -23.13263232891376\n",
      "epoch =  69 batch =  50 / 63 loss =  -23.121554468192308\n",
      "Validation loss =  -23.02635955810547\n",
      "Effective sample size =  0.16904\n",
      "epoch =  70 batch =  0 / 63 loss =  -23.241802215576172\n",
      "epoch =  70 batch =  25 / 63 loss =  -23.10629353156457\n",
      "epoch =  70 batch =  50 / 63 loss =  -23.128745770921896\n",
      "Validation loss =  -23.023773193359375\n",
      "Effective sample size =  0.156361\n",
      "epoch =  71 batch =  0 / 63 loss =  -23.018375396728516\n",
      "epoch =  71 batch =  25 / 63 loss =  -23.107207518357498\n",
      "epoch =  71 batch =  50 / 63 loss =  -23.12294010087555\n",
      "Validation loss =  -23.05532455444336\n",
      "Effective sample size =  0.181021\n",
      "epoch =  72 batch =  0 / 63 loss =  -23.2298583984375\n",
      "epoch =  72 batch =  25 / 63 loss =  -23.12522712120643\n",
      "epoch =  72 batch =  50 / 63 loss =  -23.130775788251093\n",
      "Validation loss =  -23.051359176635742\n",
      "Effective sample size =  0.157384\n",
      "epoch =  73 batch =  0 / 63 loss =  -23.192323684692383\n",
      "epoch =  73 batch =  25 / 63 loss =  -23.03405497624324\n",
      "epoch =  73 batch =  50 / 63 loss =  -23.084854724360447\n",
      "Validation loss =  -22.972749710083008\n",
      "Effective sample size =  0.154698\n",
      "epoch =  74 batch =  0 / 63 loss =  -23.06942367553711\n",
      "epoch =  74 batch =  25 / 63 loss =  -23.090710786672737\n",
      "epoch =  74 batch =  50 / 63 loss =  -23.098836674409753\n",
      "Validation loss =  -23.01777458190918\n",
      "Effective sample size =  0.169862\n",
      "epoch =  75 batch =  0 / 63 loss =  -22.96676254272461\n",
      "epoch =  75 batch =  25 / 63 loss =  -23.117866882911095\n",
      "epoch =  75 batch =  50 / 63 loss =  -23.136163075764973\n",
      "Validation loss =  -23.020368576049805\n",
      "Effective sample size =  0.171188\n",
      "epoch =  76 batch =  0 / 63 loss =  -23.12310791015625\n",
      "epoch =  76 batch =  25 / 63 loss =  -23.157952382014347\n",
      "epoch =  76 batch =  50 / 63 loss =  -23.16033591476141\n",
      "Validation loss =  -23.05660057067871\n",
      "Effective sample size =  0.184039\n",
      "epoch =  77 batch =  0 / 63 loss =  -23.113365173339844\n",
      "epoch =  77 batch =  25 / 63 loss =  -23.127709975609413\n",
      "epoch =  77 batch =  50 / 63 loss =  -23.130816553153245\n",
      "Validation loss =  -22.924909591674805\n",
      "Effective sample size =  0.136701\n",
      "epoch =  78 batch =  0 / 63 loss =  -23.030067443847656\n",
      "epoch =  78 batch =  25 / 63 loss =  -23.111831885117752\n",
      "epoch =  78 batch =  50 / 63 loss =  -23.1193421681722\n",
      "Validation loss =  -22.937808990478516\n",
      "Effective sample size =  0.135171\n",
      "epoch =  79 batch =  0 / 63 loss =  -22.987783432006836\n",
      "epoch =  79 batch =  25 / 63 loss =  -23.13980616056002\n",
      "epoch =  79 batch =  50 / 63 loss =  -23.140071457507563\n",
      "Validation loss =  -22.912813186645508\n",
      "Effective sample size =  0.13641\n",
      "epoch =  80 batch =  0 / 63 loss =  -23.04812240600586\n",
      "epoch =  80 batch =  25 / 63 loss =  -23.116564530592697\n",
      "epoch =  80 batch =  50 / 63 loss =  -23.11653200785319\n",
      "Validation loss =  -22.907278060913086\n",
      "Effective sample size =  0.139607\n",
      "epoch =  81 batch =  0 / 63 loss =  -23.13189697265625\n",
      "epoch =  81 batch =  25 / 63 loss =  -23.11479267707238\n",
      "epoch =  81 batch =  50 / 63 loss =  -23.118784698785515\n",
      "Validation loss =  -23.07097625732422\n",
      "Effective sample size =  0.184722\n",
      "epoch =  82 batch =  0 / 63 loss =  -23.103801727294922\n",
      "epoch =  82 batch =  25 / 63 loss =  -23.12749803983248\n",
      "epoch =  82 batch =  50 / 63 loss =  -23.12147308798397\n",
      "Validation loss =  -22.938386917114258\n",
      "Effective sample size =  0.144977\n",
      "epoch =  83 batch =  0 / 63 loss =  -23.346010208129883\n",
      "epoch =  83 batch =  25 / 63 loss =  -23.14883752969595\n",
      "epoch =  83 batch =  50 / 63 loss =  -23.14330441344018\n",
      "Validation loss =  -23.072160720825195\n",
      "Effective sample size =  0.185321\n",
      "epoch =  84 batch =  0 / 63 loss =  -23.065807342529297\n",
      "epoch =  84 batch =  25 / 63 loss =  -23.14326623769907\n",
      "epoch =  84 batch =  50 / 63 loss =  -23.125434763291302\n",
      "Validation loss =  -22.961816787719727\n",
      "Effective sample size =  0.148291\n",
      "epoch =  85 batch =  0 / 63 loss =  -23.011276245117188\n",
      "epoch =  85 batch =  25 / 63 loss =  -23.173598509568436\n",
      "epoch =  85 batch =  50 / 63 loss =  -23.160045623779297\n",
      "Validation loss =  -23.05940055847168\n",
      "Effective sample size =  0.177568\n",
      "epoch =  86 batch =  0 / 63 loss =  -23.077049255371094\n",
      "epoch =  86 batch =  25 / 63 loss =  -23.18821012056791\n",
      "epoch =  86 batch =  50 / 63 loss =  -23.162961024864046\n",
      "Validation loss =  -23.015087127685547\n",
      "Effective sample size =  0.158454\n",
      "epoch =  87 batch =  0 / 63 loss =  -22.948949813842773\n",
      "epoch =  87 batch =  25 / 63 loss =  -23.133524674635666\n",
      "epoch =  87 batch =  50 / 63 loss =  -23.14822552250881\n",
      "Validation loss =  -22.982046127319336\n",
      "Effective sample size =  0.160949\n",
      "epoch =  88 batch =  0 / 63 loss =  -22.941133499145508\n",
      "epoch =  88 batch =  25 / 63 loss =  -23.131096546466534\n",
      "epoch =  88 batch =  50 / 63 loss =  -23.14027576820524\n",
      "Validation loss =  -23.06584930419922\n",
      "Effective sample size =  0.188037\n",
      "epoch =  89 batch =  0 / 63 loss =  -23.22565460205078\n",
      "epoch =  89 batch =  25 / 63 loss =  -23.15101902301495\n",
      "epoch =  89 batch =  50 / 63 loss =  -23.165499631096335\n",
      "Validation loss =  -23.07849884033203\n",
      "Effective sample size =  0.186598\n",
      "epoch =  90 batch =  0 / 63 loss =  -23.19918441772461\n",
      "epoch =  90 batch =  25 / 63 loss =  -23.091560803926907\n",
      "epoch =  90 batch =  50 / 63 loss =  -23.136657752242744\n",
      "Validation loss =  -23.066740036010742\n",
      "Effective sample size =  0.181641\n",
      "epoch =  91 batch =  0 / 63 loss =  -23.098495483398438\n",
      "epoch =  91 batch =  25 / 63 loss =  -23.102114310631386\n",
      "epoch =  91 batch =  50 / 63 loss =  -23.118204752604168\n",
      "Validation loss =  -23.04681968688965\n",
      "Effective sample size =  0.171651\n",
      "epoch =  92 batch =  0 / 63 loss =  -23.223407745361328\n",
      "epoch =  92 batch =  25 / 63 loss =  -23.20596196101262\n",
      "epoch =  92 batch =  50 / 63 loss =  -23.166719436645508\n",
      "Validation loss =  -23.063575744628906\n",
      "Effective sample size =  0.178854\n",
      "epoch =  93 batch =  0 / 63 loss =  -23.168716430664062\n",
      "epoch =  93 batch =  25 / 63 loss =  -23.186866760253903\n",
      "epoch =  93 batch =  50 / 63 loss =  -23.168767068900305\n",
      "Validation loss =  -23.077735900878906\n",
      "Effective sample size =  0.18386\n",
      "epoch =  94 batch =  0 / 63 loss =  -23.343109130859375\n",
      "epoch =  94 batch =  25 / 63 loss =  -23.141277386591984\n",
      "epoch =  94 batch =  50 / 63 loss =  -23.14112622130151\n",
      "Validation loss =  -23.109987258911133\n",
      "Effective sample size =  0.19834\n",
      "epoch =  95 batch =  0 / 63 loss =  -23.277984619140625\n",
      "epoch =  95 batch =  25 / 63 loss =  -23.193012971144455\n",
      "epoch =  95 batch =  50 / 63 loss =  -23.182127746881225\n",
      "Validation loss =  -23.061641693115234\n",
      "Effective sample size =  0.18032\n",
      "epoch =  96 batch =  0 / 63 loss =  -23.211332321166992\n",
      "epoch =  96 batch =  25 / 63 loss =  -23.178907027611366\n",
      "epoch =  96 batch =  50 / 63 loss =  -23.146220973893712\n",
      "Validation loss =  -22.965211868286133\n",
      "Effective sample size =  0.152662\n",
      "epoch =  97 batch =  0 / 63 loss =  -23.237415313720703\n",
      "epoch =  97 batch =  25 / 63 loss =  -23.14780770815336\n",
      "epoch =  97 batch =  50 / 63 loss =  -23.14553956424489\n",
      "Validation loss =  -23.08319854736328\n",
      "Effective sample size =  0.189821\n",
      "epoch =  98 batch =  0 / 63 loss =  -23.31760025024414\n",
      "epoch =  98 batch =  25 / 63 loss =  -23.182500325716457\n",
      "epoch =  98 batch =  50 / 63 loss =  -23.160326303220256\n",
      "Validation loss =  -23.074275970458984\n",
      "Effective sample size =  0.188042\n",
      "epoch =  99 batch =  0 / 63 loss =  -23.136898040771484\n",
      "epoch =  99 batch =  25 / 63 loss =  -23.17246935917781\n",
      "epoch =  99 batch =  50 / 63 loss =  -23.145025365492877\n",
      "Validation loss =  -22.915374755859375\n",
      "Effective sample size =  0.116518\n",
      "epoch =  100 batch =  0 / 63 loss =  -23.081775665283203\n",
      "epoch =  100 batch =  25 / 63 loss =  -23.219371135418232\n",
      "epoch =  100 batch =  50 / 63 loss =  -23.193710214951462\n",
      "Validation loss =  -22.941795349121094\n",
      "Effective sample size =  0.125101\n",
      "epoch =  101 batch =  0 / 63 loss =  -23.076988220214844\n",
      "epoch =  101 batch =  25 / 63 loss =  -23.169248580932617\n",
      "epoch =  101 batch =  50 / 63 loss =  -23.153346192603028\n",
      "Validation loss =  -23.048336029052734\n",
      "Effective sample size =  0.160017\n",
      "epoch =  102 batch =  0 / 63 loss =  -23.22408103942871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  102 batch =  25 / 63 loss =  -23.167951803940994\n",
      "epoch =  102 batch =  50 / 63 loss =  -23.145322724884632\n",
      "Validation loss =  -23.097145080566406\n",
      "Effective sample size =  0.193805\n",
      "epoch =  103 batch =  0 / 63 loss =  -23.26131820678711\n",
      "epoch =  103 batch =  25 / 63 loss =  -23.150866215045635\n",
      "epoch =  103 batch =  50 / 63 loss =  -23.139224781709558\n",
      "Validation loss =  -23.08690071105957\n",
      "Effective sample size =  0.184646\n",
      "epoch =  104 batch =  0 / 63 loss =  -23.300537109375\n",
      "epoch =  104 batch =  25 / 63 loss =  -23.180049969599796\n",
      "epoch =  104 batch =  50 / 63 loss =  -23.177788379145603\n",
      "Validation loss =  -22.971410751342773\n",
      "Effective sample size =  0.155994\n",
      "epoch =  105 batch =  0 / 63 loss =  -22.91793441772461\n",
      "epoch =  105 batch =  25 / 63 loss =  -23.210590802706204\n",
      "epoch =  105 batch =  50 / 63 loss =  -23.20238629509421\n",
      "Validation loss =  -23.098913192749023\n",
      "Effective sample size =  0.192206\n",
      "epoch =  106 batch =  0 / 63 loss =  -23.408803939819336\n",
      "epoch =  106 batch =  25 / 63 loss =  -23.16973693554218\n",
      "epoch =  106 batch =  50 / 63 loss =  -23.15736437778847\n",
      "Validation loss =  -23.009767532348633\n",
      "Effective sample size =  0.167564\n",
      "epoch =  107 batch =  0 / 63 loss =  -23.02286148071289\n",
      "epoch =  107 batch =  25 / 63 loss =  -23.163583828852726\n",
      "epoch =  107 batch =  50 / 63 loss =  -23.186139649035884\n",
      "Validation loss =  -22.990751266479492\n",
      "Effective sample size =  0.156479\n",
      "epoch =  108 batch =  0 / 63 loss =  -23.06421661376953\n",
      "epoch =  108 batch =  25 / 63 loss =  -23.16276851067176\n",
      "epoch =  108 batch =  50 / 63 loss =  -23.173305025287704\n",
      "Validation loss =  -23.005483627319336\n",
      "Effective sample size =  0.152385\n",
      "epoch =  109 batch =  0 / 63 loss =  -23.17238426208496\n",
      "epoch =  109 batch =  25 / 63 loss =  -23.11581802368164\n",
      "epoch =  109 batch =  50 / 63 loss =  -23.121951982086788\n",
      "Validation loss =  -22.966035842895508\n"
     ]
    }
   ],
   "source": [
    "data_size = train_samples.shape[0]\n",
    "n_batches = m.ceil(data_size/batch_size)\n",
    "\n",
    "data_size_validation = test_samples.shape[0]\n",
    "n_batches_validate = m.ceil(data_size_validation/batch_size)\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "best_ess = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(data_size, device=device)    \n",
    "\n",
    "    # Loop over batches\n",
    "    cum_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # Set up the batch\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end   = min( (batch+1)*batch_size, data_size-1 )\n",
    "        indices = permutation[batch_begin:batch_end]\n",
    "        samples_batch = train_samples[indices]\n",
    "        weights_batch = train_weights[indices]\n",
    "        \n",
    "        # Take a step\n",
    "        optimizer.zero_grad()\n",
    "        loss = -(flow.log_prob(inputs=samples_batch)*weights_batch).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute cumulative loss\n",
    "        cum_loss = (cum_loss*batch + loss.item())/(batch+1)\n",
    "\n",
    "        if batch%25 == 0:\n",
    "            print(\"epoch = \", epoch, \"batch = \", batch, \"/\", n_batches, \"loss = \", cum_loss)\n",
    "    \n",
    "    writer.add_scalar(\"Loss_train\", cum_loss, epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    # ---------- Compute validation loss -----------\n",
    "    validation_loss = 0\n",
    "    for batch in range(n_batches_validate):\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end = min( (batch+1)*batch_size, data_size_validation-1 )\n",
    "        samples_batch = test_samples[batch_begin:batch_end]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            validation_loss = (validation_loss*batch - (flow.log_prob(samples_batch)).mean())/(batch+1)\n",
    "\n",
    "    print(\"Validation loss = \", validation_loss.item())\n",
    "    writer.add_scalar(\"Loss_test\", validation_loss.item(), epoch)\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        torch.save(flow, \"flow_model_weighted_{}_best_validation.pt\".format(reference_method))\n",
    "        best_validation_loss = validation_loss\n",
    "\n",
    "    \n",
    "    # ---------- Compute effective sample size ----------\n",
    "    # generate samples and evaluate llhs\n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(n_sample)\n",
    "        llhs = flow.log_prob(samples)\n",
    "\n",
    "    # Store files\n",
    "    np.savetxt(\"/tmp/samples_weighted_file.csv\", samples.cpu().numpy(), delimiter=',')\n",
    "    np.savetxt(\"/tmp/llhs_weighted_file.csv\", np.exp(llhs.cpu().numpy()), delimiter=',')\n",
    "\n",
    "    # Run the evaluator\n",
    "    cmd = os.path.abspath(os.getcwd())+'/ME_VEGAS/compute_metrics_from_likelihoods /tmp/samples_weighted_file.csv /tmp/llhs_weighted_file.csv'\n",
    "    b = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE).stdout\n",
    "    lines = b.decode('ascii').split(\"\\n\")\n",
    "\n",
    "    ess = float(lines[2].split(' ')[-1])\n",
    "    \n",
    "    print(\"Effective sample size = \", ess)\n",
    "    writer.add_scalar(\"Effective_sample_size\", ess, epoch)\n",
    "\n",
    "    if ess > best_ess:\n",
    "        torch.save(flow, \"flow_model_weighted_{}_best_ess.pt\".format(reference_method))\n",
    "        best_ess = ess\n",
    "        \n",
    "torch.save(flow, \"flow_model_weighted_{}_final.pt\".format(reference_method))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
