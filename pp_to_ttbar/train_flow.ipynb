{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import random as r\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows.base import Flow\n",
    "from nflows.distributions.uniform import BoxUniform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseRationalQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.autoregressive import MaskedPiecewiseQuadraticAutoregressiveTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.permutations import RandomPermutation\n",
    "from nflows.transforms.splines.rational_quadratic import rational_quadratic_spline\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import math as m\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard writer for loss logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU/CPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_RQS_knots = 16   # Number of knots in RQS transform\n",
    "n_made_layers = 3  # Number of hidden layers in every made network\n",
    "n_made_units = 200 # Number of units in every layer of the made network\n",
    "n_flow_layers = 8  # Number of layers in the flow\n",
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 800\n",
    "adam_lr = 0.001   # Learning rate for the ADAM optimizer (default: 0.001)\n",
    "\n",
    "n_train = int(1e6) # This is missing the required statistical factor to account for negative weights\n",
    "n_test = int(1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.genfromtxt(\"data/negative_weight_samples.csv\", delimiter=',')[:,:8]\n",
    "weights = np.genfromtxt(\"data/negative_weight_weights.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7995490, 8)\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the fraction of negative events and the required statistical factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of negative events 0.2394467380986031\n",
      "Statistical factor 3.6825358174692755\n"
     ]
    }
   ],
   "source": [
    "f = np.sum(weights < 0)/len(weights)\n",
    "c = (1-2*f)**-2\n",
    "print(\"Fraction of negative events\", f)\n",
    "print(\"Statistical factor\", c)\n",
    "if (n_train + n_test)*c > samples.shape[0]:\n",
    "    raise Exception(\"Not enough training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.sign(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train_with_stats = int(n_train*c)\n",
    "n_test_with_stats = int(n_test*c)\n",
    "\n",
    "train_samples = torch.tensor(samples[:n_train_with_stats], dtype=torch.float32, device=device)\n",
    "train_weights = torch.tensor(weights[:n_train_with_stats], dtype=torch.float32, device=device)\n",
    "test_samples = torch.tensor(samples[n_train_with_stats:n_train_with_stats+n_test_with_stats], dtype=torch.float32, device=device)\n",
    "test_weights = torch.tensor(weights[n_train_with_stats:n_train_with_stats+n_test_with_stats], dtype=torch.float32, device=device)\n",
    "\n",
    "del samples\n",
    "del weights\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dim = train_samples.shape[1]\n",
    "base_dist = BoxUniform(torch.zeros(event_dim), torch.ones(event_dim))\n",
    "\n",
    "transforms = []\n",
    "for _ in range(n_flow_layers):\n",
    "    transforms.append(RandomPermutation(features=event_dim))\n",
    "    transforms.append(MaskedPiecewiseRationalQuadraticAutoregressiveTransform(\n",
    "        features=event_dim, \n",
    "        hidden_features=n_made_units,\n",
    "        num_bins=n_RQS_knots,\n",
    "        num_blocks=n_made_layers-1,\n",
    "        tails=\"constrained\",\n",
    "        use_residual_blocks=False\n",
    "    ))\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist).to(device)\n",
    "optimizer = optim.Adam(flow.parameters(), lr=adam_lr)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[350, 425, 500, 575, 650, 725, 800], gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 batch =  0 / 3597 loss =  3.236827850341797\n",
      "epoch =  0 batch =  25 / 3597 loss =  -0.5628551657383258\n",
      "epoch =  0 batch =  50 / 3597 loss =  -1.5663445486741905\n",
      "epoch =  0 batch =  75 / 3597 loss =  -2.1702778511925747\n",
      "epoch =  0 batch =  100 / 3597 loss =  -2.530372174659578\n",
      "epoch =  0 batch =  125 / 3597 loss =  -2.7436772916052075\n",
      "epoch =  0 batch =  150 / 3597 loss =  -2.9068196917211764\n",
      "epoch =  0 batch =  175 / 3597 loss =  -3.043420422483574\n",
      "epoch =  0 batch =  200 / 3597 loss =  -3.151381560819066\n",
      "epoch =  0 batch =  225 / 3597 loss =  -3.252307253073802\n",
      "epoch =  0 batch =  250 / 3597 loss =  -3.3355220983702822\n",
      "epoch =  0 batch =  275 / 3597 loss =  -3.404759450667146\n",
      "epoch =  0 batch =  300 / 3597 loss =  -3.463435308877812\n",
      "epoch =  0 batch =  325 / 3597 loss =  -3.5197199784173554\n",
      "epoch =  0 batch =  350 / 3597 loss =  -3.5701684051769074\n",
      "epoch =  0 batch =  375 / 3597 loss =  -3.6196508899014046\n",
      "epoch =  0 batch =  400 / 3597 loss =  -3.6584784904323016\n",
      "epoch =  0 batch =  425 / 3597 loss =  -3.69440273247974\n",
      "epoch =  0 batch =  450 / 3597 loss =  -3.7256656800564008\n",
      "epoch =  0 batch =  475 / 3597 loss =  -3.7643516396774963\n",
      "epoch =  0 batch =  500 / 3597 loss =  -3.795673840774034\n",
      "epoch =  0 batch =  525 / 3597 loss =  -3.827767881830382\n",
      "epoch =  0 batch =  550 / 3597 loss =  -3.85916445108161\n",
      "epoch =  0 batch =  575 / 3597 loss =  -3.885487398960524\n",
      "epoch =  0 batch =  600 / 3597 loss =  -3.911372653458162\n",
      "epoch =  0 batch =  625 / 3597 loss =  -3.9359423777165885\n",
      "epoch =  0 batch =  650 / 3597 loss =  -3.957149380912429\n",
      "epoch =  0 batch =  675 / 3597 loss =  -3.978603007701727\n",
      "epoch =  0 batch =  700 / 3597 loss =  -4.0030382800204265\n",
      "epoch =  0 batch =  725 / 3597 loss =  -4.021903534730275\n",
      "epoch =  0 batch =  750 / 3597 loss =  -4.04174030335067\n",
      "epoch =  0 batch =  775 / 3597 loss =  -4.060726975196417\n",
      "epoch =  0 batch =  800 / 3597 loss =  -4.076526315768857\n",
      "epoch =  0 batch =  825 / 3597 loss =  -4.0907220888080085\n",
      "epoch =  0 batch =  850 / 3597 loss =  -4.102937509114257\n",
      "epoch =  0 batch =  875 / 3597 loss =  -4.119326587817441\n",
      "epoch =  0 batch =  900 / 3597 loss =  -4.134820728667172\n",
      "epoch =  0 batch =  925 / 3597 loss =  -4.147942878901317\n",
      "epoch =  0 batch =  950 / 3597 loss =  -4.162255128350043\n",
      "epoch =  0 batch =  975 / 3597 loss =  -4.174844630551147\n",
      "epoch =  0 batch =  1000 / 3597 loss =  -4.186770564549933\n",
      "epoch =  0 batch =  1025 / 3597 loss =  -4.198279279598254\n",
      "epoch =  0 batch =  1050 / 3597 loss =  -4.207744064498926\n",
      "epoch =  0 batch =  1075 / 3597 loss =  -4.217486776494629\n",
      "epoch =  0 batch =  1100 / 3597 loss =  -4.229147269766948\n",
      "epoch =  0 batch =  1125 / 3597 loss =  -4.237836115843029\n",
      "epoch =  0 batch =  1150 / 3597 loss =  -4.245409197956456\n",
      "epoch =  0 batch =  1175 / 3597 loss =  -4.252646901270973\n",
      "epoch =  0 batch =  1200 / 3597 loss =  -4.261532411984265\n",
      "epoch =  0 batch =  1225 / 3597 loss =  -4.267780926352036\n",
      "epoch =  0 batch =  1250 / 3597 loss =  -4.274719871872435\n",
      "epoch =  0 batch =  1275 / 3597 loss =  -4.28190515304808\n",
      "epoch =  0 batch =  1300 / 3597 loss =  -4.2899943106363265\n",
      "epoch =  0 batch =  1325 / 3597 loss =  -4.298990300787705\n",
      "epoch =  0 batch =  1350 / 3597 loss =  -4.3055226734882845\n",
      "epoch =  0 batch =  1375 / 3597 loss =  -4.313789159492699\n",
      "epoch =  0 batch =  1400 / 3597 loss =  -4.3201212822582615\n",
      "epoch =  0 batch =  1425 / 3597 loss =  -4.328807849619867\n",
      "epoch =  0 batch =  1450 / 3597 loss =  -4.3348023092064825\n",
      "epoch =  0 batch =  1475 / 3597 loss =  -4.338992808327118\n",
      "epoch =  0 batch =  1500 / 3597 loss =  -4.344101154907475\n",
      "epoch =  0 batch =  1525 / 3597 loss =  -4.3489040516775805\n",
      "epoch =  0 batch =  1550 / 3597 loss =  -4.3544226929420455\n",
      "epoch =  0 batch =  1575 / 3597 loss =  -4.359410175892901\n",
      "epoch =  0 batch =  1600 / 3597 loss =  -4.365690521491006\n",
      "epoch =  0 batch =  1625 / 3597 loss =  -4.371750910563655\n",
      "epoch =  0 batch =  1650 / 3597 loss =  -4.377083610939583\n",
      "epoch =  0 batch =  1675 / 3597 loss =  -4.382430741283368\n",
      "epoch =  0 batch =  1700 / 3597 loss =  -4.388262363337962\n",
      "epoch =  0 batch =  1725 / 3597 loss =  -4.392838361296423\n",
      "epoch =  0 batch =  1750 / 3597 loss =  -4.397518473649146\n",
      "epoch =  0 batch =  1775 / 3597 loss =  -4.402203033434915\n",
      "epoch =  0 batch =  1800 / 3597 loss =  -4.406827772610191\n",
      "epoch =  0 batch =  1825 / 3597 loss =  -4.409863498816745\n",
      "epoch =  0 batch =  1850 / 3597 loss =  -4.415602268173135\n",
      "epoch =  0 batch =  1875 / 3597 loss =  -4.419649120269295\n",
      "epoch =  0 batch =  1900 / 3597 loss =  -4.423317476487795\n",
      "epoch =  0 batch =  1925 / 3597 loss =  -4.426690377859802\n",
      "epoch =  0 batch =  1950 / 3597 loss =  -4.430841747459414\n",
      "epoch =  0 batch =  1975 / 3597 loss =  -4.43522820317069\n",
      "epoch =  0 batch =  2000 / 3597 loss =  -4.438500778666734\n",
      "epoch =  0 batch =  2025 / 3597 loss =  -4.443359824947398\n",
      "epoch =  0 batch =  2050 / 3597 loss =  -4.44652158658368\n",
      "epoch =  0 batch =  2075 / 3597 loss =  -4.450145517527938\n",
      "epoch =  0 batch =  2100 / 3597 loss =  -4.452775319549705\n",
      "epoch =  0 batch =  2125 / 3597 loss =  -4.4557847883044115\n",
      "epoch =  0 batch =  2150 / 3597 loss =  -4.458730143039846\n",
      "epoch =  0 batch =  2175 / 3597 loss =  -4.461868321468282\n",
      "epoch =  0 batch =  2200 / 3597 loss =  -4.46555033758086\n",
      "epoch =  0 batch =  2225 / 3597 loss =  -4.468611761191244\n",
      "epoch =  0 batch =  2250 / 3597 loss =  -4.471273205164852\n",
      "epoch =  0 batch =  2275 / 3597 loss =  -4.473717699622854\n",
      "epoch =  0 batch =  2300 / 3597 loss =  -4.476254934904003\n",
      "epoch =  0 batch =  2325 / 3597 loss =  -4.47873527457359\n",
      "epoch =  0 batch =  2350 / 3597 loss =  -4.480976068481692\n",
      "epoch =  0 batch =  2375 / 3597 loss =  -4.484090874341599\n",
      "epoch =  0 batch =  2400 / 3597 loss =  -4.486904286335325\n",
      "epoch =  0 batch =  2425 / 3597 loss =  -4.489641424898064\n",
      "epoch =  0 batch =  2450 / 3597 loss =  -4.492160396398884\n",
      "epoch =  0 batch =  2475 / 3597 loss =  -4.494708149084556\n",
      "epoch =  0 batch =  2500 / 3597 loss =  -4.496576203626895\n",
      "epoch =  0 batch =  2525 / 3597 loss =  -4.498665988256501\n",
      "epoch =  0 batch =  2550 / 3597 loss =  -4.500954225034342\n",
      "epoch =  0 batch =  2575 / 3597 loss =  -4.502892783756206\n",
      "epoch =  0 batch =  2600 / 3597 loss =  -4.50556602431278\n",
      "epoch =  0 batch =  2625 / 3597 loss =  -4.508070385964512\n",
      "epoch =  0 batch =  2650 / 3597 loss =  -4.509493664362344\n",
      "epoch =  0 batch =  2675 / 3597 loss =  -4.511791234460105\n",
      "epoch =  0 batch =  2700 / 3597 loss =  -4.513885593952753\n",
      "epoch =  0 batch =  2725 / 3597 loss =  -4.515993161710492\n",
      "epoch =  0 batch =  2750 / 3597 loss =  -4.5181442895225\n",
      "epoch =  0 batch =  2775 / 3597 loss =  -4.5201054160893746\n",
      "epoch =  0 batch =  2800 / 3597 loss =  -4.522024146704441\n",
      "epoch =  0 batch =  2825 / 3597 loss =  -4.523516043524991\n",
      "epoch =  0 batch =  2850 / 3597 loss =  -4.525454484592681\n",
      "epoch =  0 batch =  2875 / 3597 loss =  -4.527283185090394\n",
      "epoch =  0 batch =  2900 / 3597 loss =  -4.529066240857349\n",
      "epoch =  0 batch =  2925 / 3597 loss =  -4.531094175817528\n",
      "epoch =  0 batch =  2950 / 3597 loss =  -4.5318219107880156\n",
      "epoch =  0 batch =  2975 / 3597 loss =  -4.533201982377342\n",
      "epoch =  0 batch =  3000 / 3597 loss =  -4.534419704659372\n",
      "epoch =  0 batch =  3025 / 3597 loss =  -4.535838119217818\n",
      "epoch =  0 batch =  3050 / 3597 loss =  -4.537388227799399\n",
      "epoch =  0 batch =  3075 / 3597 loss =  -4.539222830875007\n",
      "epoch =  0 batch =  3100 / 3597 loss =  -4.540531785478121\n",
      "epoch =  0 batch =  3125 / 3597 loss =  -4.5422123902818665\n",
      "epoch =  0 batch =  3150 / 3597 loss =  -4.543820040031518\n",
      "epoch =  0 batch =  3175 / 3597 loss =  -4.5451883012686825\n",
      "epoch =  0 batch =  3200 / 3597 loss =  -4.546789986198663\n",
      "epoch =  0 batch =  3225 / 3597 loss =  -4.5487951661442105\n",
      "epoch =  0 batch =  3250 / 3597 loss =  -4.550360926588795\n",
      "epoch =  0 batch =  3275 / 3597 loss =  -4.551438745234027\n",
      "epoch =  0 batch =  3300 / 3597 loss =  -4.553113141842794\n",
      "epoch =  0 batch =  3325 / 3597 loss =  -4.555717990366853\n",
      "epoch =  0 batch =  3350 / 3597 loss =  -4.557514431120126\n",
      "epoch =  0 batch =  3375 / 3597 loss =  -4.5594453222308005\n",
      "epoch =  0 batch =  3400 / 3597 loss =  -4.561316475416753\n",
      "epoch =  0 batch =  3425 / 3597 loss =  -4.562603282670968\n",
      "epoch =  0 batch =  3450 / 3597 loss =  -4.564407235019543\n",
      "epoch =  0 batch =  3475 / 3597 loss =  -4.565208122117047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 batch =  3500 / 3597 loss =  -4.565562218912855\n",
      "epoch =  0 batch =  3525 / 3597 loss =  -4.566952724552789\n",
      "epoch =  0 batch =  3550 / 3597 loss =  -4.568034326315117\n",
      "epoch =  0 batch =  3575 / 3597 loss =  -4.570064053272763\n",
      "Validation loss =  -4.777291297912598\n",
      "epoch =  1 batch =  0 / 3597 loss =  -4.485310077667236\n",
      "epoch =  1 batch =  25 / 3597 loss =  -4.765618782777053\n",
      "epoch =  1 batch =  50 / 3597 loss =  -4.768677178551169\n",
      "epoch =  1 batch =  75 / 3597 loss =  -4.768467627073589\n",
      "epoch =  1 batch =  100 / 3597 loss =  -4.798830084281391\n",
      "epoch =  1 batch =  125 / 3597 loss =  -4.797404497388808\n",
      "epoch =  1 batch =  150 / 3597 loss =  -4.791207376694835\n",
      "epoch =  1 batch =  175 / 3597 loss =  -4.79660339518027\n",
      "epoch =  1 batch =  200 / 3597 loss =  -4.788188614062408\n",
      "epoch =  1 batch =  225 / 3597 loss =  -4.7744540978322005\n",
      "epoch =  1 batch =  250 / 3597 loss =  -4.78298174337562\n",
      "epoch =  1 batch =  275 / 3597 loss =  -4.776787820069687\n",
      "epoch =  1 batch =  300 / 3597 loss =  -4.772068218535366\n",
      "epoch =  1 batch =  325 / 3597 loss =  -4.770708771571063\n",
      "epoch =  1 batch =  350 / 3597 loss =  -4.7682141578435235\n",
      "epoch =  1 batch =  375 / 3597 loss =  -4.770530104637148\n",
      "epoch =  1 batch =  400 / 3597 loss =  -4.7700928179105935\n",
      "epoch =  1 batch =  425 / 3597 loss =  -4.769658058462012\n",
      "epoch =  1 batch =  450 / 3597 loss =  -4.771385585125167\n",
      "epoch =  1 batch =  475 / 3597 loss =  -4.77108891270742\n",
      "epoch =  1 batch =  500 / 3597 loss =  -4.772169734665498\n",
      "epoch =  1 batch =  525 / 3597 loss =  -4.770381642838397\n",
      "epoch =  1 batch =  550 / 3597 loss =  -4.7681795231010895\n",
      "epoch =  1 batch =  575 / 3597 loss =  -4.7680308454566545\n",
      "epoch =  1 batch =  600 / 3597 loss =  -4.767793609377947\n",
      "epoch =  1 batch =  625 / 3597 loss =  -4.769122602078862\n",
      "epoch =  1 batch =  650 / 3597 loss =  -4.769961236991824\n",
      "epoch =  1 batch =  675 / 3597 loss =  -4.769637332160093\n",
      "epoch =  1 batch =  700 / 3597 loss =  -4.767029445963816\n",
      "epoch =  1 batch =  725 / 3597 loss =  -4.770447072247169\n",
      "epoch =  1 batch =  750 / 3597 loss =  -4.769438517553985\n",
      "epoch =  1 batch =  775 / 3597 loss =  -4.770289411249845\n",
      "epoch =  1 batch =  800 / 3597 loss =  -4.772413793723379\n",
      "epoch =  1 batch =  825 / 3597 loss =  -4.773443984638976\n",
      "epoch =  1 batch =  850 / 3597 loss =  -4.773366690522491\n",
      "epoch =  1 batch =  875 / 3597 loss =  -4.776243205484186\n",
      "epoch =  1 batch =  900 / 3597 loss =  -4.777197303306245\n",
      "epoch =  1 batch =  925 / 3597 loss =  -4.777698545167561\n",
      "epoch =  1 batch =  950 / 3597 loss =  -4.779073352191974\n",
      "epoch =  1 batch =  975 / 3597 loss =  -4.780510660077701\n",
      "epoch =  1 batch =  1000 / 3597 loss =  -4.7798518758196415\n",
      "epoch =  1 batch =  1025 / 3597 loss =  -4.78044090958831\n",
      "epoch =  1 batch =  1050 / 3597 loss =  -4.779374505541188\n",
      "epoch =  1 batch =  1075 / 3597 loss =  -4.778280616692891\n",
      "epoch =  1 batch =  1100 / 3597 loss =  -4.777367019739936\n",
      "epoch =  1 batch =  1125 / 3597 loss =  -4.777528510424\n",
      "epoch =  1 batch =  1150 / 3597 loss =  -4.775039972790833\n",
      "epoch =  1 batch =  1175 / 3597 loss =  -4.7740558669680615\n",
      "epoch =  1 batch =  1200 / 3597 loss =  -4.773982422437197\n",
      "epoch =  1 batch =  1225 / 3597 loss =  -4.773515708683753\n",
      "epoch =  1 batch =  1250 / 3597 loss =  -4.772725004276973\n",
      "epoch =  1 batch =  1275 / 3597 loss =  -4.768943431235406\n",
      "epoch =  1 batch =  1300 / 3597 loss =  -4.769874617468845\n",
      "epoch =  1 batch =  1325 / 3597 loss =  -4.771117880693213\n",
      "epoch =  1 batch =  1350 / 3597 loss =  -4.769452877171745\n",
      "epoch =  1 batch =  1375 / 3597 loss =  -4.7707795358674545\n",
      "epoch =  1 batch =  1400 / 3597 loss =  -4.77071636119628\n",
      "epoch =  1 batch =  1425 / 3597 loss =  -4.771275631675887\n",
      "epoch =  1 batch =  1450 / 3597 loss =  -4.772976187161138\n",
      "epoch =  1 batch =  1475 / 3597 loss =  -4.772344809560599\n",
      "epoch =  1 batch =  1500 / 3597 loss =  -4.773478181103245\n",
      "epoch =  1 batch =  1525 / 3597 loss =  -4.774569348301517\n",
      "epoch =  1 batch =  1550 / 3597 loss =  -4.7744467324552655\n",
      "epoch =  1 batch =  1575 / 3597 loss =  -4.774990463317348\n",
      "epoch =  1 batch =  1600 / 3597 loss =  -4.774647451206567\n",
      "epoch =  1 batch =  1625 / 3597 loss =  -4.774437038954188\n",
      "epoch =  1 batch =  1650 / 3597 loss =  -4.7743687450633105\n",
      "epoch =  1 batch =  1675 / 3597 loss =  -4.775499061354595\n",
      "epoch =  1 batch =  1700 / 3597 loss =  -4.775122835662779\n",
      "epoch =  1 batch =  1725 / 3597 loss =  -4.7770440318769625\n",
      "epoch =  1 batch =  1750 / 3597 loss =  -4.776047601759749\n",
      "epoch =  1 batch =  1775 / 3597 loss =  -4.776033085477253\n",
      "epoch =  1 batch =  1800 / 3597 loss =  -4.775360435462549\n",
      "epoch =  1 batch =  1825 / 3597 loss =  -4.777017119419152\n",
      "epoch =  1 batch =  1850 / 3597 loss =  -4.775916568399579\n",
      "epoch =  1 batch =  1875 / 3597 loss =  -4.775781460916572\n",
      "epoch =  1 batch =  1900 / 3597 loss =  -4.77549633209735\n",
      "epoch =  1 batch =  1925 / 3597 loss =  -4.775575567752533\n",
      "epoch =  1 batch =  1950 / 3597 loss =  -4.775424604107949\n",
      "epoch =  1 batch =  1975 / 3597 loss =  -4.774970905500875\n",
      "epoch =  1 batch =  2000 / 3597 loss =  -4.775726835707443\n",
      "epoch =  1 batch =  2025 / 3597 loss =  -4.776085181156252\n",
      "epoch =  1 batch =  2050 / 3597 loss =  -4.77581010987735\n",
      "epoch =  1 batch =  2075 / 3597 loss =  -4.776488371437463\n",
      "epoch =  1 batch =  2100 / 3597 loss =  -4.776553293343452\n",
      "epoch =  1 batch =  2125 / 3597 loss =  -4.777232232232878\n",
      "epoch =  1 batch =  2150 / 3597 loss =  -4.777309175204927\n",
      "epoch =  1 batch =  2175 / 3597 loss =  -4.776979645604602\n",
      "epoch =  1 batch =  2200 / 3597 loss =  -4.776733602517742\n",
      "epoch =  1 batch =  2225 / 3597 loss =  -4.777275012831165\n",
      "epoch =  1 batch =  2250 / 3597 loss =  -4.776752207555651\n",
      "epoch =  1 batch =  2275 / 3597 loss =  -4.775881691641049\n",
      "epoch =  1 batch =  2300 / 3597 loss =  -4.777090720641314\n",
      "epoch =  1 batch =  2325 / 3597 loss =  -4.7765051191056145\n",
      "epoch =  1 batch =  2350 / 3597 loss =  -4.776077373744789\n",
      "epoch =  1 batch =  2375 / 3597 loss =  -4.776172379451978\n",
      "epoch =  1 batch =  2400 / 3597 loss =  -4.775749255001232\n",
      "epoch =  1 batch =  2425 / 3597 loss =  -4.775332308562463\n",
      "epoch =  1 batch =  2450 / 3597 loss =  -4.774803807131381\n",
      "epoch =  1 batch =  2475 / 3597 loss =  -4.775667402008634\n",
      "epoch =  1 batch =  2500 / 3597 loss =  -4.776306060732305\n",
      "epoch =  1 batch =  2525 / 3597 loss =  -4.776056850796558\n",
      "epoch =  1 batch =  2550 / 3597 loss =  -4.7763957681024225\n",
      "epoch =  1 batch =  2575 / 3597 loss =  -4.7764054732108105\n",
      "epoch =  1 batch =  2600 / 3597 loss =  -4.776826704982989\n",
      "epoch =  1 batch =  2625 / 3597 loss =  -4.776938939130873\n",
      "epoch =  1 batch =  2650 / 3597 loss =  -4.776430335327273\n",
      "epoch =  1 batch =  2675 / 3597 loss =  -4.776362469467185\n",
      "epoch =  1 batch =  2700 / 3597 loss =  -4.775580917892978\n",
      "epoch =  1 batch =  2725 / 3597 loss =  -4.776791940344153\n",
      "epoch =  1 batch =  2750 / 3597 loss =  -4.776674103624226\n",
      "epoch =  1 batch =  2775 / 3597 loss =  -4.777180987493463\n",
      "epoch =  1 batch =  2800 / 3597 loss =  -4.777218426350666\n",
      "epoch =  1 batch =  2825 / 3597 loss =  -4.777892261638101\n",
      "epoch =  1 batch =  2850 / 3597 loss =  -4.77824865253882\n",
      "epoch =  1 batch =  2875 / 3597 loss =  -4.778297359240416\n",
      "epoch =  1 batch =  2900 / 3597 loss =  -4.777791382156141\n",
      "epoch =  1 batch =  2925 / 3597 loss =  -4.778745677889992\n",
      "epoch =  1 batch =  2950 / 3597 loss =  -4.779976690070028\n",
      "epoch =  1 batch =  2975 / 3597 loss =  -4.779880227821498\n",
      "epoch =  1 batch =  3000 / 3597 loss =  -4.77930107604501\n",
      "epoch =  1 batch =  3025 / 3597 loss =  -4.779144953909266\n",
      "epoch =  1 batch =  3050 / 3597 loss =  -4.778609296682439\n",
      "epoch =  1 batch =  3075 / 3597 loss =  -4.77867095298359\n",
      "epoch =  1 batch =  3100 / 3597 loss =  -4.779004768416491\n",
      "epoch =  1 batch =  3125 / 3597 loss =  -4.779564509312474\n",
      "epoch =  1 batch =  3150 / 3597 loss =  -4.779895758182433\n",
      "epoch =  1 batch =  3175 / 3597 loss =  -4.780662269301024\n",
      "epoch =  1 batch =  3200 / 3597 loss =  -4.781896910642097\n",
      "epoch =  1 batch =  3225 / 3597 loss =  -4.781629060486869\n",
      "epoch =  1 batch =  3250 / 3597 loss =  -4.780686975222158\n",
      "epoch =  1 batch =  3275 / 3597 loss =  -4.780738239104942\n",
      "epoch =  1 batch =  3300 / 3597 loss =  -4.781038738243231\n",
      "epoch =  1 batch =  3325 / 3597 loss =  -4.781155105176212\n",
      "epoch =  1 batch =  3350 / 3597 loss =  -4.780821813252115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1 batch =  3375 / 3597 loss =  -4.781784652180591\n",
      "epoch =  1 batch =  3400 / 3597 loss =  -4.78300388719503\n",
      "epoch =  1 batch =  3425 / 3597 loss =  -4.783600207385593\n",
      "epoch =  1 batch =  3450 / 3597 loss =  -4.784383395277093\n",
      "epoch =  1 batch =  3475 / 3597 loss =  -4.784761549482696\n",
      "epoch =  1 batch =  3500 / 3597 loss =  -4.784735959994738\n",
      "epoch =  1 batch =  3525 / 3597 loss =  -4.785197779122101\n",
      "epoch =  1 batch =  3550 / 3597 loss =  -4.785001984182404\n",
      "epoch =  1 batch =  3575 / 3597 loss =  -4.785553956258494\n",
      "Validation loss =  -4.815684795379639\n",
      "epoch =  2 batch =  0 / 3597 loss =  -5.163106441497803\n",
      "epoch =  2 batch =  25 / 3597 loss =  -4.769309777479906\n",
      "epoch =  2 batch =  50 / 3597 loss =  -4.753867990830365\n",
      "epoch =  2 batch =  75 / 3597 loss =  -4.773739588888066\n",
      "epoch =  2 batch =  100 / 3597 loss =  -4.754236466813794\n",
      "epoch =  2 batch =  125 / 3597 loss =  -4.773835553063285\n",
      "epoch =  2 batch =  150 / 3597 loss =  -4.7789648368658595\n",
      "epoch =  2 batch =  175 / 3597 loss =  -4.7752869792959896\n",
      "epoch =  2 batch =  200 / 3597 loss =  -4.778233949224746\n",
      "epoch =  2 batch =  225 / 3597 loss =  -4.780846052465184\n",
      "epoch =  2 batch =  250 / 3597 loss =  -4.777436050285854\n",
      "epoch =  2 batch =  275 / 3597 loss =  -4.7740476019140585\n",
      "epoch =  2 batch =  300 / 3597 loss =  -4.779316310470681\n",
      "epoch =  2 batch =  325 / 3597 loss =  -4.78070182961189\n",
      "epoch =  2 batch =  350 / 3597 loss =  -4.784779322453033\n",
      "epoch =  2 batch =  375 / 3597 loss =  -4.793182309003587\n",
      "epoch =  2 batch =  400 / 3597 loss =  -4.792084665964367\n",
      "epoch =  2 batch =  425 / 3597 loss =  -4.790346994646276\n",
      "epoch =  2 batch =  450 / 3597 loss =  -4.790395938107812\n",
      "epoch =  2 batch =  475 / 3597 loss =  -4.791267588859845\n",
      "epoch =  2 batch =  500 / 3597 loss =  -4.791783374226734\n",
      "epoch =  2 batch =  525 / 3597 loss =  -4.790014455073684\n",
      "epoch =  2 batch =  550 / 3597 loss =  -4.793358472644092\n",
      "epoch =  2 batch =  575 / 3597 loss =  -4.792980142351652\n",
      "epoch =  2 batch =  600 / 3597 loss =  -4.794743260607346\n",
      "epoch =  2 batch =  625 / 3597 loss =  -4.7938758843242155\n",
      "epoch =  2 batch =  650 / 3597 loss =  -4.79693103935312\n",
      "epoch =  2 batch =  675 / 3597 loss =  -4.797536480003558\n",
      "epoch =  2 batch =  700 / 3597 loss =  -4.797882844990222\n",
      "epoch =  2 batch =  725 / 3597 loss =  -4.799469944859332\n",
      "epoch =  2 batch =  750 / 3597 loss =  -4.802332095553802\n",
      "epoch =  2 batch =  775 / 3597 loss =  -4.803365900037208\n",
      "epoch =  2 batch =  800 / 3597 loss =  -4.8018020169714015\n",
      "epoch =  2 batch =  825 / 3597 loss =  -4.8008611447585885\n",
      "epoch =  2 batch =  850 / 3597 loss =  -4.802064304766438\n",
      "epoch =  2 batch =  875 / 3597 loss =  -4.80173982305613\n",
      "epoch =  2 batch =  900 / 3597 loss =  -4.802537423524415\n",
      "epoch =  2 batch =  925 / 3597 loss =  -4.802706234655928\n",
      "epoch =  2 batch =  950 / 3597 loss =  -4.8009563828617665\n",
      "epoch =  2 batch =  975 / 3597 loss =  -4.800845502097091\n",
      "epoch =  2 batch =  1000 / 3597 loss =  -4.799819750266587\n",
      "epoch =  2 batch =  1025 / 3597 loss =  -4.800968008664144\n",
      "epoch =  2 batch =  1050 / 3597 loss =  -4.802460200666354\n",
      "epoch =  2 batch =  1075 / 3597 loss =  -4.80262777419781\n",
      "epoch =  2 batch =  1100 / 3597 loss =  -4.803017088546191\n",
      "epoch =  2 batch =  1125 / 3597 loss =  -4.8043850342714896\n",
      "epoch =  2 batch =  1150 / 3597 loss =  -4.805539702664853\n",
      "epoch =  2 batch =  1175 / 3597 loss =  -4.8060387650720005\n",
      "epoch =  2 batch =  1200 / 3597 loss =  -4.804688553528214\n",
      "epoch =  2 batch =  1225 / 3597 loss =  -4.804638883806934\n",
      "epoch =  2 batch =  1250 / 3597 loss =  -4.8046034727927385\n",
      "epoch =  2 batch =  1275 / 3597 loss =  -4.804956675883735\n",
      "epoch =  2 batch =  1300 / 3597 loss =  -4.8033519797284745\n",
      "epoch =  2 batch =  1325 / 3597 loss =  -4.801925012069225\n",
      "epoch =  2 batch =  1350 / 3597 loss =  -4.8005108136234185\n",
      "epoch =  2 batch =  1375 / 3597 loss =  -4.799825809202909\n",
      "epoch =  2 batch =  1400 / 3597 loss =  -4.8015864334473966\n",
      "epoch =  2 batch =  1425 / 3597 loss =  -4.800882883192274\n",
      "epoch =  2 batch =  1450 / 3597 loss =  -4.800738779944762\n"
     ]
    }
   ],
   "source": [
    "data_size = train_samples.shape[0]\n",
    "n_batches = m.ceil(data_size/batch_size)\n",
    "\n",
    "data_size_validation = test_samples.shape[0]\n",
    "n_batches_validate = m.ceil(data_size_validation/batch_size)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    permutation = torch.randperm(data_size, device=device)    \n",
    "\n",
    "    # Loop over batches\n",
    "    cum_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # Set up the batch\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end   = min( (batch+1)*batch_size, data_size-1 )\n",
    "        indices = permutation[batch_begin:batch_end]\n",
    "        samples_batch = train_samples[indices]\n",
    "        weights_batch = train_weights[indices]\n",
    "        \n",
    "        # Take a step\n",
    "        optimizer.zero_grad()\n",
    "        loss = -(flow.log_prob(inputs=samples_batch)*weights_batch).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute cumulative loss\n",
    "        cum_loss = (cum_loss*batch + loss.item())/(batch+1)\n",
    "\n",
    "        if batch%25 == 0:\n",
    "            print(\"epoch = \", epoch, \"batch = \", batch, \"/\", n_batches, \"loss = \", cum_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Compute validation loss\n",
    "    validation_loss = 0\n",
    "    for batch in range(n_batches_validate):\n",
    "        batch_begin = batch*batch_size\n",
    "        batch_end = min( (batch+1)*batch_size, data_size_validation-1 )\n",
    "        samples_batch = test_samples[batch_begin:batch_end]\n",
    "        weights_batch = test_weights[batch_begin:batch_end]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            validation_loss = (validation_loss*batch - (flow.log_prob(samples_batch)*weights_batch).mean())/(batch+1)\n",
    "    \n",
    "    print(\"Validation loss = \", validation_loss.item())\n",
    "    \n",
    "    writer.add_scalar(\"Loss_train\", cum_loss, epoch)\n",
    "    writer.add_scalar(\"Loss_test\", validation_loss, epoch)\n",
    "    \n",
    "    if validation_loss < best_loss:\n",
    "        torch.save(flow, \"best_flow_model.pt\")\n",
    "        best_loss = validation_loss\n",
    "\n",
    "torch.save(flow, \"final_flow_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
